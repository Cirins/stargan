{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49913,
     "status": "ok",
     "timestamp": 1732700095290,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "I0_1KsKpCPtk",
    "outputId": "d5d4b745-321e-4168-c4e8-9d6b432ed3e2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd drive/MyDrive/ST/stargan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLVVBGiQmD5W"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9665,
     "status": "ok",
     "timestamp": 1732700104951,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "yh05UGFkCPtm"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import copy\n",
    "\n",
    "seed = 2710\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732700104952,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "UJN2bKbaCPtm"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'realworld': {\n",
    "        'dataset_name': 'realworld',\n",
    "        'num_df_domains': 10,\n",
    "        'num_dp_domains': 5,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['WAL', 'RUN', 'CLD', 'CLU'],\n",
    "        'num_timesteps': 128,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 4,\n",
    "    },\n",
    "    'cwru': {\n",
    "        'dataset_name': 'cwru_256_3ch_5cl',\n",
    "        'num_df_domains': 4,\n",
    "        'num_dp_domains': 4,\n",
    "        'num_classes': 5,\n",
    "        'class_names': ['IR', 'Ball', 'OR_centred', 'OR_orthogonal', 'OR_opposite'],\n",
    "        'num_timesteps': 256,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 5,\n",
    "    },\n",
    "    'realworld_mobiact': {\n",
    "        'dataset_name': 'realworld_mobiact',\n",
    "        'num_df_domains': 15,\n",
    "        'num_dp_domains': 61,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['WAL', 'RUN', 'CLD', 'CLU'],\n",
    "        'num_timesteps': 128,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 4,\n",
    "    },\n",
    "    'mobiact_realworld': {\n",
    "        'dataset_name': 'mobiact_realworld',\n",
    "        'num_df_domains': 61,\n",
    "        'num_dp_domains': 15,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['WAL', 'RUN', 'CLD', 'CLU'],\n",
    "        'num_timesteps': 128,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 4,\n",
    "    }\n",
    "}\n",
    "\n",
    "syn_name = 'rwma0311'\n",
    "num_epochs = 1000\n",
    "patience = 20\n",
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732700104953,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "XwSwaF-LCPtn"
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, domains_set, src_class, domain=None, rot=False):\n",
    "    # Load configurations\n",
    "    dataset_name = config[dataset]['dataset_name']\n",
    "    class_idx = config[dataset]['class_names'].index(src_class)\n",
    "    num_df_domains = config[dataset]['num_df_domains']\n",
    "\n",
    "    if rot:\n",
    "        dataset_name += '_rot'\n",
    "\n",
    "    if domains_set == 'df':\n",
    "        dataset_name += '_Df'\n",
    "    elif domains_set == 'dp':\n",
    "        dataset_name += '_Dp'\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid domains set: {domains_set}\")\n",
    "\n",
    "    try:\n",
    "        with open(f'../../data/{dataset_name}.pkl', 'rb') as f:\n",
    "            x, y, k = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Dataset files for {dataset} not found.\")\n",
    "\n",
    "    # Additional domain filtering if specified\n",
    "    if domain is not None:\n",
    "        domain_mask = (k == domain)\n",
    "        x, y, k = x[domain_mask], y[domain_mask], k[domain_mask]\n",
    "\n",
    "    assert len(x) > 0, f\"No data found\"\n",
    "    assert len(x) == len(y) == len(k), f\"Data length mismatch\"\n",
    "\n",
    "    return x, y, k\n",
    "\n",
    "\n",
    "\n",
    "def get_syn_data(dataset, syn_name, src_class, domain=None, fs=False):\n",
    "    # Load configurations\n",
    "    class_names = config[dataset]['class_names']\n",
    "\n",
    "    try:\n",
    "        with open(f'data/{dataset}_{syn_name}.pkl', 'rb') as f:\n",
    "            x, y, k = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Dataset files for {dataset} not found.\")\n",
    "\n",
    "    with open(f'data/{dataset}_{syn_name}_fs.pkl', 'rb') as f:\n",
    "        fs = pickle.load(f)\n",
    "\n",
    "    x, y, k = x[fs == 0], y[fs == 0], k[fs == 0]\n",
    "\n",
    "    if domain is not None:\n",
    "        domain_mask = (k == domain)\n",
    "        x, y, k = x[domain_mask], y[domain_mask], k[domain_mask]\n",
    "\n",
    "    assert len(x) > 0, f\"No data found\"\n",
    "    assert len(x) == len(y) == len(k), f\"Data length mismatch\"\n",
    "\n",
    "    return x, y, k\n",
    "\n",
    "\n",
    "\n",
    "class TSTRClassifier(nn.Module):\n",
    "    def __init__(self, num_timesteps=128, num_channels=3, num_classes=5):\n",
    "        super(TSTRClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.fc_shared = nn.Linear(num_timesteps * 8, 100)\n",
    "\n",
    "        self.fc_class = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc_shared(x))\n",
    "\n",
    "        # Final output for class prediction\n",
    "        class_outputs = self.fc_class(x)\n",
    "        return class_outputs\n",
    "\n",
    "\n",
    "\n",
    "def remap_labels(y):\n",
    "    label_map = {clss: i for i, clss in enumerate(np.unique(y))}\n",
    "    return np.array([label_map[clss] for clss in y])\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(x, y, shuffle=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    y = remap_labels(y)\n",
    "    y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "    dataset = TensorDataset(x, y)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=shuffle)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def random_rotation_matrix():\n",
    "    \"\"\"Generate a random rotation matrix from a predefined set of quaternions.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Randomly generate a quaternion\n",
    "    q = np.random.rand(4)\n",
    "\n",
    "    # Convert quaternion to rotation matrix\n",
    "    q = torch.tensor(q, device=device, dtype=torch.float32)\n",
    "    q = q / torch.norm(q)  # Normalize quaternion\n",
    "    q0, q1, q2, q3 = q\n",
    "\n",
    "    R = torch.tensor([\n",
    "        [1 - 2*q2**2 - 2*q3**2, 2*q1*q2 - 2*q3*q0, 2*q1*q3 + 2*q2*q0],\n",
    "        [2*q1*q2 + 2*q3*q0, 1 - 2*q1**2 - 2*q3**2, 2*q2*q3 - 2*q1*q0],\n",
    "        [2*q1*q3 - 2*q2*q0, 2*q2*q3 + 2*q1*q0, 1 - 2*q1**2 - 2*q2**2]\n",
    "    ], device=device, dtype=torch.float32)\n",
    "\n",
    "    return R, q\n",
    "\n",
    "\n",
    "def augment_batch(x_real):\n",
    "    \"\"\"Apply random rotation to the batch of real time series.\"\"\"\n",
    "    min_val, max_val = -19.61, 19.61\n",
    "    x_real = x_real * (max_val - min_val) + min_val  # De-normalize\n",
    "    R, q = random_rotation_matrix()\n",
    "    x_real = torch.matmul(R, x_real)  # Apply rotation\n",
    "    x_real = (x_real - min_val) / (max_val - min_val)  # Re-normalize\n",
    "    return x_real, q\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted_labels.detach().cpu().numpy())\n",
    "            all_labels.extend(y_batch.detach().cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    total_loss /= len(test_loader)\n",
    "\n",
    "    return accuracy, total_loss, f1\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=100, augment=False, patience=-1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    accuracy_val = []\n",
    "\n",
    "    best_model_state = None\n",
    "    best_loss = np.inf\n",
    "    best_accuracy = 0\n",
    "    best_f1 = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Set up linear learning rate decay\n",
    "    lambda_lr = lambda epoch: 1 - epoch / num_epochs\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            if augment:\n",
    "                x_batch, _ = augment_batch(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_loader)\n",
    "        loss_train.append(total_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        val_accuracy, val_loss, val_f1 = evaluate_model(model, val_loader)\n",
    "        if val_loss < best_loss:\n",
    "            best_epoch = epoch\n",
    "            best_accuracy = val_accuracy\n",
    "            best_f1 = val_f1\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        loss_val.append(val_loss)\n",
    "        accuracy_val.append(val_accuracy)\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"\\tEpoch {epoch + 1}/{num_epochs} - Train loss: {total_loss:.4f} - Val loss: {val_loss:.4f} - Val accuracy: {val_accuracy:.4f} - Val F1: {val_f1:.4f} - LR: {current_lr:.2e}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience > 0 and epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\tBest epoch: {best_epoch + 1} - Best val loss: {best_loss:.4f} - Best val accuracy: {best_accuracy:.4f} - Best val F1: {best_f1:.4f}\\n\")\n",
    "\n",
    "    # Load best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_and_test(x_train, y_train, x_test, y_test, dataset, num_epochs=100, augment=False):\n",
    "    assert np.array_equal(np.unique(y_train), np.unique(y_test)), \"Training and test labels do not match\"\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "    test_loader = get_dataloader(x_test, y_test)\n",
    "\n",
    "    model = TSTRClassifier(num_timesteps=config[dataset]['num_timesteps'],\n",
    "                           num_channels=config[dataset]['num_channels'],\n",
    "                           num_classes=config[dataset]['num_classes'])\n",
    "    initial_lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs, augment)\n",
    "\n",
    "    test_accuracy, test_loss, test_f1 = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "    return test_accuracy, test_loss, test_f1\n",
    "\n",
    "\n",
    "\n",
    "def train_only(x_train, y_train, dataset, num_epochs=100, augment=False, patience=-1):\n",
    "\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "\n",
    "    model = TSTRClassifier(num_timesteps=config[dataset]['num_timesteps'],\n",
    "                           num_channels=config[dataset]['num_channels'],\n",
    "                           num_classes=num_classes)\n",
    "    initial_lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs, augment=augment, patience=patience)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "\n",
    "def train_classifier_cv(x_train, y_train, dataset, num_epochs=100):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    accs = []\n",
    "    losses = []\n",
    "    f1s = []\n",
    "\n",
    "    for train_index, test_index in skf.split(x_train, y_train):\n",
    "        x_train_fold, x_test_fold = x_train[train_index], x_train[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        acc, loss, f1 = train_and_test(x_train_fold, y_train_fold, x_test_fold, y_test_fold, dataset, num_epochs)\n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "        f1s.append(f1)\n",
    "    return np.mean(accs), np.mean(losses), np.mean(f1s)\n",
    "\n",
    "\n",
    "\n",
    "def fine_tune(model, x_train, y_train, num_epochs=100):\n",
    "    # Freeze feature extraction layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv' in name or 'bn' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "\n",
    "    initial_lr = 0.00001\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "\n",
    "def save_scores(source, domain, accuracy, loss, f1, name, dataset):\n",
    "    results_dir = 'results'\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Path to the CSV file\n",
    "    file_path = os.path.join(results_dir, f'{dataset}_{name}.csv')\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    # Open the file in append mode if it exists, or write mode if it doesn't\n",
    "    with open(file_path, mode='a' if file_exists else 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # If the file does not exist, write the header\n",
    "        if not file_exists:\n",
    "            writer.writerow(['source', 'domain', 'accuracy', 'loss', 'f1'])\n",
    "        # Write the data rows\n",
    "        writer.writerow([source, domain, accuracy, loss, f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRAlLTPumD5Z"
   },
   "source": [
    "# TSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1312028,
     "status": "ok",
     "timestamp": 1732701416976,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "XrDIDrL9CPtn",
    "outputId": "6b6ff457-cf86-4be8-8a0b-644a90f58b8c"
   },
   "outputs": [],
   "source": [
    "# def compute_TSTR_Dp(dataset):\n",
    "#     accs = []\n",
    "#     f1s = []\n",
    "\n",
    "#     for src_class in config[dataset]['class_names']:\n",
    "#         if src_class != 'WAL':\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "#         for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "#             print(f\"Domain: {domain}\")\n",
    "\n",
    "#             if domain == 74:\n",
    "#               continue\n",
    "\n",
    "#             # Load Dp data\n",
    "#             x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "#             print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "#             # Train and evaluate via cross-validation on Dp data\n",
    "#             print('Training and evaluating on Dp data via cross-validation...')\n",
    "#             acc, loss, f1 = train_classifier_cv(x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "#             save_scores(src_class, domain, acc, loss, f1, 'Dp', dataset)\n",
    "#             accs.append(acc)\n",
    "#             f1s.append(f1)\n",
    "#             print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "#     print(f\"Mean accuracy: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "#     print(f\"Mean F1: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Dp('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732701416976,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "M7S9IU2VVAmx"
   },
   "outputs": [],
   "source": [
    "# def compute_TSTR_Dpc(dataset):\n",
    "\n",
    "#     raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Dpc('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 959343,
     "status": "ok",
     "timestamp": 1732702376314,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "zhTuwDtYCPto",
    "outputId": "977ab2fe-a8fe-4d8b-da30-ab9d1168314c"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Df(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(num_runs):\n",
    "\n",
    "        accs_run = []\n",
    "        f1s_run = []\n",
    "\n",
    "        for src_class in config[dataset]['class_names']:\n",
    "            if src_class != 'WAL':\n",
    "                continue\n",
    "\n",
    "            print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "            # Load Df data\n",
    "            x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "            print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)} | np.unique(k_df): {np.unique(k_df)}\\n')\n",
    "\n",
    "            # Train on Df data\n",
    "            print('Training on Df data...')\n",
    "            df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs, patience=patience)\n",
    "\n",
    "            for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "                print(f\"Domain: {domain}\")\n",
    "\n",
    "                # Load Dp data\n",
    "                x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "                print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)} | np.unique(k_dp_dom): {np.unique(k_dp_dom)}')\n",
    "\n",
    "                # Evaluate on Dp data\n",
    "                acc, loss, f1 = evaluate_model(df_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "                save_scores(src_class, domain, acc, loss, f1, 'Df', dataset)\n",
    "                accs_run.append(acc)\n",
    "                f1s_run.append(f1)\n",
    "                print(f'Source class: {src_class} | Domain: {domain} | Loss: {loss:.4f} | Accuracy: {acc:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "        print(f\"Mean accuracy for run {i}: {np.mean(accs_run):.4f} +- {np.std(accs_run):.4f}\")\n",
    "        print(f\"Mean F1 for run {i}: {np.mean(f1s_run):.4f} +- {np.std(f1s_run):.4f}\\n\")\n",
    "        accs.append(np.mean(accs_run))\n",
    "        f1s.append(np.mean(f1s_run))\n",
    "\n",
    "    print(f\"Mean accuracy over {num_runs} runs: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1 over {num_runs} runs: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1158975,
     "status": "ok",
     "timestamp": 1732703535261,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "6X3sBMvtdTCl",
    "outputId": "aff1604a-d2a8-46ab-8328-6bb3f60f8a1c"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Df_aug(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(num_runs):\n",
    "\n",
    "        accs_run = []\n",
    "        f1s_run = []\n",
    "\n",
    "        for src_class in config[dataset]['class_names']:\n",
    "            if src_class != 'WAL':\n",
    "                continue\n",
    "\n",
    "            print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "            # Load Df data\n",
    "            x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "            print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)} | np.unique(k_df): {np.unique(k_df)}\\n')\n",
    "\n",
    "            # Train on Df data\n",
    "            print('Training on Df data...')\n",
    "            df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs, augment=True, patience=patience)\n",
    "\n",
    "            for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "                print(f\"Domain: {domain}\")\n",
    "\n",
    "                # Load Dp data\n",
    "                x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "                print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)} | np.unique(k_dp_dom): {np.unique(k_dp_dom)}')\n",
    "\n",
    "                # Evaluate on Dp data\n",
    "                acc, loss, f1 = evaluate_model(df_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "                save_scores(src_class, domain, acc, loss, f1, 'Df_aug', dataset)\n",
    "                accs_run.append(acc)\n",
    "                f1s_run.append(f1)\n",
    "                print(f'Source class: {src_class} | Domain: {domain} | Loss: {loss:.4f} | Accuracy: {acc:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "        print(f\"Mean accuracy for run {i}: {np.mean(accs_run):.4f} +- {np.std(accs_run):.4f}\")\n",
    "        print(f\"Mean F1 for run {i}: {np.mean(f1s_run):.4f} +- {np.std(f1s_run):.4f}\\n\")\n",
    "        accs.append(np.mean(accs_run))\n",
    "        f1s.append(np.mean(f1s_run))\n",
    "\n",
    "    print(f\"Mean accuracy over {num_runs} runs: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1 over {num_runs} runs: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df_aug('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1732703535262,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "aDSS4eIiUHeq"
   },
   "outputs": [],
   "source": [
    "# def compute_TSTR_Df_rot(dataset):\n",
    "#     accs = []\n",
    "#     f1s = []\n",
    "\n",
    "#     for i in range(num_runs):\n",
    "\n",
    "#         accs_run = []\n",
    "#         f1s_run = []\n",
    "\n",
    "#         for src_class in config[dataset]['class_names']:\n",
    "#             if src_class != 'WAL':\n",
    "#                 continue\n",
    "\n",
    "#             print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "#             # Load Df data\n",
    "#             x_df, y_df, k_df = get_data(dataset, 'df', src_class, rot=True)\n",
    "#             print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "#             # Train on Df data\n",
    "#             print('Training on Df data...')\n",
    "#             df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "#             for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "#                 print(f\"Domain: {domain}\")\n",
    "\n",
    "#                 # Load Dp data\n",
    "#                 x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "#                 print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "#                 # Evaluate on Dp data\n",
    "#                 acc, loss, f1 = evaluate_model(df_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "#                 save_scores(src_class, domain, acc, loss, f1, 'Df_rot', dataset)\n",
    "#                 accs_run.append(acc)\n",
    "#                 f1s_run.append(f1)\n",
    "#                 print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "#         print(f\"Mean accuracy for run {i}: {np.mean(accs_run):.4f}\")\n",
    "#         print(f\"Mean F1 for run {i}: {np.mean(f1s_run):.4f}\\n\")\n",
    "#         accs.append(np.mean(accs_run))\n",
    "#         f1s.append(np.mean(f1s_run))\n",
    "\n",
    "#     print(f\"Mean accuracy over {num_runs} runs: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "#     print(f\"Mean F1 over {num_runs} runs: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Df_rot('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370702,
     "status": "ok",
     "timestamp": 1732703905936,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "SmGQm1ZFCPto",
    "outputId": "c0383541-29e1-4419-d140-1f75be295f6d"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Syn(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            y0_indices_syn = np.where(y_syn_dom == 0)[0]\n",
    "            y0_indices_dp = np.where(y_dp_dom == 0)[0]\n",
    "            assert np.array_equal(y0_indices_syn, y0_indices_dp), \"Labels do not match\"\n",
    "            assert np.array_equal(y_syn_dom[y0_indices_syn], y_dp_dom[y0_indices_dp]), \"Labels do not match\"\n",
    "\n",
    "            y0_indices_train, y0_indices_test = train_test_split(y0_indices_syn, test_size=0.5, shuffle=True, random_state=seed)\n",
    "\n",
    "            x_syn_dom_y0, y_syn_dom_y0 = x_syn_dom[y0_indices_train], y_syn_dom[y0_indices_train]\n",
    "            x_dp_dom_y0, y_dp_dom_y0 = x_dp_dom[y0_indices_test], y_dp_dom[y0_indices_test]\n",
    "\n",
    "            x_syn_dom = np.concatenate([x_syn_dom_y0, x_syn_dom[y_syn_dom != 0]])\n",
    "            y_syn_dom = np.concatenate([y_syn_dom_y0, y_syn_dom[y_syn_dom != 0]])\n",
    "            x_dp_dom = np.concatenate([x_dp_dom_y0, x_dp_dom[y_dp_dom != 0]])\n",
    "            y_dp_dom = np.concatenate([y_dp_dom_y0, y_dp_dom[y_dp_dom != 0]])\n",
    "\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss, f1 = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451058,
     "status": "ok",
     "timestamp": 1732704356973,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "iKHIjEk_0q0O",
    "outputId": "decb455c-847a-42e6-8b1f-2338b2249288"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Syn_aug(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            y0_indices_syn = np.where(y_syn_dom == 0)[0]\n",
    "            y0_indices_dp = np.where(y_dp_dom == 0)[0]\n",
    "            assert np.array_equal(y0_indices_syn, y0_indices_dp), \"Labels do not match\"\n",
    "            assert np.array_equal(y_syn_dom[y0_indices_syn], y_dp_dom[y0_indices_dp]), \"Labels do not match\"\n",
    "\n",
    "            y0_indices_train, y0_indices_test = train_test_split(y0_indices_syn, test_size=0.5, shuffle=True, random_state=seed)\n",
    "\n",
    "            x_syn_dom_y0, y_syn_dom_y0 = x_syn_dom[y0_indices_train], y_syn_dom[y0_indices_train]\n",
    "            x_dp_dom_y0, y_dp_dom_y0 = x_dp_dom[y0_indices_test], y_dp_dom[y0_indices_test]\n",
    "\n",
    "            x_syn_dom = np.concatenate([x_syn_dom_y0, x_syn_dom[y_syn_dom != 0]])\n",
    "            y_syn_dom = np.concatenate([y_syn_dom_y0, y_syn_dom[y_syn_dom != 0]])\n",
    "            x_dp_dom = np.concatenate([x_dp_dom_y0, x_dp_dom[y_dp_dom != 0]])\n",
    "            y_dp_dom = np.concatenate([y_dp_dom_y0, y_dp_dom[y_dp_dom != 0]])\n",
    "\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss, f1 = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs, augment=True)\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Syn_aug', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Syn_aug('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1732704356974,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "KKvjK_PAMxLm"
   },
   "outputs": [],
   "source": [
    "# def compute_TSTR_Syn_all(dataset):\n",
    "#     accs = []\n",
    "\n",
    "#     for src_class in config[dataset]['class_names']:\n",
    "#         if src_class != 'WAL':\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "#         # Load synthetic data\n",
    "#         x_syn, y_syn, k_syn = get_syn_data(dataset, syn_name, src_class)\n",
    "#         print(f'x_syn.shape: {x_syn.shape} | np.unique(y_syn): {np.unique(y_syn)}')\n",
    "\n",
    "#         # Load Dp data\n",
    "#         x_dp, y_dp, k_dp = get_data(dataset, 'dp', src_class)\n",
    "#         print(f'x_dp.shape: {x_dp.shape} | np.unique(y_dp): {np.unique(y_dp)}\\n')\n",
    "\n",
    "#         # Train on synthetic data and evaluate on Dp data\n",
    "#         print('Training on synthetic data...')\n",
    "#         acc, loss = train_and_test(x_syn, y_syn, x_dp, y_dp, dataset, num_epochs=num_epochs)\n",
    "#         save_scores(src_class, 100, acc, loss, 'Syn_all', dataset)\n",
    "#         accs.append(acc)\n",
    "#         print(f'Source class: {src_class} | Domain: All | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "#     print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Syn_all('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328032,
     "status": "ok",
     "timestamp": 1732704684983,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "LA71th1bCPto",
    "outputId": "4c7a1752-e271-44cb-8e3b-ec94ec86cbe9"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Df_Syn(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            y0_indices_syn = np.where(y_syn_dom == 0)[0]\n",
    "            y0_indices_dp = np.where(y_dp_dom == 0)[0]\n",
    "            assert np.array_equal(y0_indices_syn, y0_indices_dp), \"Labels do not match\"\n",
    "            assert np.array_equal(y_syn_dom[y0_indices_syn], y_dp_dom[y0_indices_dp]), \"Labels do not match\"\n",
    "\n",
    "            y0_indices_train, y0_indices_test = train_test_split(y0_indices_syn, test_size=0.5, shuffle=True, random_state=seed)\n",
    "\n",
    "            x_syn_dom_y0, y_syn_dom_y0 = x_syn_dom[y0_indices_train], y_syn_dom[y0_indices_train]\n",
    "            x_dp_dom_y0, y_dp_dom_y0 = x_dp_dom[y0_indices_test], y_dp_dom[y0_indices_test]\n",
    "\n",
    "            x_syn_dom = np.concatenate([x_syn_dom_y0, x_syn_dom[y_syn_dom != 0]])\n",
    "            y_syn_dom = np.concatenate([y_syn_dom_y0, y_syn_dom[y_syn_dom != 0]])\n",
    "            x_dp_dom = np.concatenate([x_dp_dom_y0, x_dp_dom[y_dp_dom != 0]])\n",
    "            y_dp_dom = np.concatenate([y_dp_dom_y0, y_dp_dom[y_dp_dom != 0]])\n",
    "\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=num_epochs)\n",
    "            acc, loss, f1 = evaluate_model(df_syn_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Df_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1732704684983,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "WrQyLH7NMxLn"
   },
   "outputs": [],
   "source": [
    "# def compute_TSTR_Df_Syn_all(dataset):\n",
    "#     accs = []\n",
    "\n",
    "#     for src_class in config[dataset]['class_names']:\n",
    "#         if src_class != 'WAL':\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "#         # Load Df data\n",
    "#         x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "#         print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "#         # Train on Df data\n",
    "#         print('Training on Df data...')\n",
    "#         df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "#         # Load synthetic data\n",
    "#         x_syn, y_syn, k_syn = get_syn_data(dataset, syn_name, src_class)\n",
    "#         print(f'x_syn.shape: {x_syn.shape} | np.unique(y_syn): {np.unique(y_syn)}')\n",
    "\n",
    "#         # Load Dp data\n",
    "#         x_dp, y_dp, k_dp = get_data(dataset, 'dp', src_class)\n",
    "#         print(f'x_dp.shape: {x_dp.shape} | np.unique(y_dp): {np.unique(y_dp)}\\n')\n",
    "\n",
    "#         # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "#         print('Fine-tuning on synthetic data...')\n",
    "#         df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn, y_syn, num_epochs=num_epochs)\n",
    "#         acc, loss = evaluate_model(df_syn_model, get_dataloader(x_dp, y_dp))\n",
    "#         save_scores(src_class, 100, acc, loss, 'Df_Syn_all', dataset)\n",
    "#         accs.append(acc)\n",
    "#         print(f'Source class: {src_class} | Domain: All | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "#     print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Df_Syn_all('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1391145,
     "status": "ok",
     "timestamp": 1732706076120,
     "user": {
      "displayName": "Pietro Castelvetri",
      "userId": "04218697278633758016"
     },
     "user_tz": -60
    },
    "id": "Y53G-W-A0q0P",
    "outputId": "c12d64a5-2191-45c7-b1c6-b952a14a3380"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Df_plus_Syn(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            y0_indices_syn = np.where(y_syn_dom == 0)[0]\n",
    "            y0_indices_dp = np.where(y_dp_dom == 0)[0]\n",
    "            assert np.array_equal(y0_indices_syn, y0_indices_dp), \"Labels do not match\"\n",
    "            assert np.array_equal(y_syn_dom[y0_indices_syn], y_dp_dom[y0_indices_dp]), \"Labels do not match\"\n",
    "\n",
    "            y0_indices_train, y0_indices_test = train_test_split(y0_indices_syn, test_size=0.5, shuffle=True, random_state=seed)\n",
    "\n",
    "            x_syn_dom_y0, y_syn_dom_y0 = x_syn_dom[y0_indices_train], y_syn_dom[y0_indices_train]\n",
    "            x_dp_dom_y0, y_dp_dom_y0 = x_dp_dom[y0_indices_test], y_dp_dom[y0_indices_test]\n",
    "\n",
    "            x_syn_dom = np.concatenate([x_syn_dom_y0, x_syn_dom[y_syn_dom != 0]])\n",
    "            y_syn_dom = np.concatenate([y_syn_dom_y0, y_syn_dom[y_syn_dom != 0]])\n",
    "            x_dp_dom = np.concatenate([x_dp_dom_y0, x_dp_dom[y_dp_dom != 0]])\n",
    "            y_dp_dom = np.concatenate([y_dp_dom_y0, y_dp_dom[y_dp_dom != 0]])\n",
    "\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Join Df and synthetic data\n",
    "            x_df_syn = np.concatenate([x_df, x_syn_dom], axis=0)\n",
    "            y_df_syn = np.concatenate([y_df, y_syn_dom], axis=0)\n",
    "            k_df_syn = np.concatenate([k_df, k_syn_dom], axis=0)\n",
    "            print(f'x_df_syn.shape: {x_df_syn.shape} | np.unique(y_df_syn): {np.unique(y_df_syn)}')\n",
    "\n",
    "            # Train on Df plus synthetic data and test on Dp data\n",
    "            print('Training on Df plus synthetic data...')\n",
    "            acc, loss, f1 = train_and_test(x_df, y_df, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Df_plus_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df_plus_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLSuTddOmD5c"
   },
   "source": [
    "# TSTRFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZqvTTwNCPto"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Dpfs data and test on Dp data\n",
    "            print('Training on Dpfs data...')\n",
    "            acc, loss = train_and_test(x_dpfs_dom, y_dpfs_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOqZB6ARCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on Dpfs data and test on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            df_dpfs_model = fine_tune(copy.deepcopy(df_model), x_dpfs_dom, y_dpfs_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(df_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA89psVZg5yb"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38LHCQjRCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(df_syn_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueNEmwhVCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Syn_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data\n",
    "            print('Training on synthetic data...')\n",
    "            syn_model = train_only(x_syn_dom, y_syn_dom, dataset, num_epochs=num_epochs)\n",
    "\n",
    "            # Fine-tune synthetic model on Dpfs data and evaluate on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            syn_dpfs_model = fine_tune(copy.deepcopy(syn_model), x_dpfs_dom, y_dpfs_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(syn_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Syn_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Syn_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z0xuuhhCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Syn_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=num_epochs)\n",
    "\n",
    "            # Fine-tune Df-syn model on Dpfs data and evaluate on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            df_syn_dpfs_model = fine_tune(copy.deepcopy(df_syn_model), x_dpfs_dom, y_dpfs_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(df_syn_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Syn_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Syn_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpB3ecNGmD5f"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_plus_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Join Df and Dpfs data\n",
    "            x_df_dpfs = np.concatenate([x_df, x_dpfs_dom], axis=0)\n",
    "            y_df_dpfs = np.concatenate([y_df, y_dpfs_dom], axis=0)\n",
    "            k_df_dpfs = np.concatenate([k_df, k_dpfs_dom], axis=0)\n",
    "            print(f'x_df_dpfs.shape: {x_df_dpfs.shape} | np.unique(y_df_dpfs): {np.unique(y_df_dpfs)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Df plus Dpfs data and test on Dp data\n",
    "            print('Training on Df plus Dpfs data...')\n",
    "            acc, loss = train_and_test(x_df_dpfs, y_df_dpfs, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_plus_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_plus_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYl0a9ZJmD5f"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_plus_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Join Df and synthetic data\n",
    "            x_df_syn = np.concatenate([x_df, x_syn_dom], axis=0)\n",
    "            y_df_syn = np.concatenate([y_df, y_syn_dom], axis=0)\n",
    "            k_df_syn = np.concatenate([k_df, k_syn_dom], axis=0)\n",
    "            print(f'x_df_syn.shape: {x_df_syn.shape} | np.unique(y_df_syn): {np.unique(y_df_syn)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Df plus synthetic data and test on Dp data\n",
    "            print('Training on Df plus synthetic data...')\n",
    "            acc, loss = train_and_test(x_df, y_df, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_plus_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_plus_Syn('realworld_mobiact')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stargan-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
