{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15916,
     "status": "ok",
     "timestamp": 1732554883374,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "I0_1KsKpCPtk",
    "outputId": "8fc2cf09-3dab-414a-86c9-5ec6b52b3ce5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd drive/MyDrive/ST/stargan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLVVBGiQmD5W"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5358,
     "status": "ok",
     "timestamp": 1732554888729,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "yh05UGFkCPtm"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import copy\n",
    "\n",
    "seed = 2710\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1732554888730,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "UJN2bKbaCPtm"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'realworld': {\n",
    "        'dataset_name': 'realworld',\n",
    "        'num_df_domains': 10,\n",
    "        'num_dp_domains': 5,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['WAL', 'RUN', 'CLD', 'CLU'],\n",
    "        'num_timesteps': 128,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 4,\n",
    "    },\n",
    "    'cwru': {\n",
    "        'dataset_name': 'cwru_256_3ch_5cl',\n",
    "        'num_df_domains': 4,\n",
    "        'num_dp_domains': 4,\n",
    "        'num_classes': 5,\n",
    "        'class_names': ['IR', 'Ball', 'OR_centred', 'OR_orthogonal', 'OR_opposite'],\n",
    "        'num_timesteps': 256,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 5,\n",
    "    },\n",
    "    'realworld_mobiact': {\n",
    "        'dataset_name': 'realworld_mobiact',\n",
    "        'num_df_domains': 15,\n",
    "        'num_dp_domains': 61,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['WAL', 'RUN', 'CLD', 'CLU'],\n",
    "        'num_timesteps': 128,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 4,\n",
    "    }\n",
    "}\n",
    "\n",
    "syn_name = 'rwma11'\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732554888730,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "XwSwaF-LCPtn"
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, domains_set, src_class, domain=None, rot=False):\n",
    "    # Load configurations\n",
    "    dataset_name = config[dataset]['dataset_name']\n",
    "    class_idx = config[dataset]['class_names'].index(src_class)\n",
    "    num_df_domains = config[dataset]['num_df_domains']\n",
    "\n",
    "    if rot:\n",
    "        dataset_name += '_rot'\n",
    "\n",
    "    try:\n",
    "        with open(f'data/{dataset_name}.pkl', 'rb') as f:\n",
    "            x, y, k = pickle.load(f)\n",
    "        with open(f'data/{dataset_name}_fs.pkl', 'rb') as f:\n",
    "            fs = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Dataset files for {dataset} not found.\")\n",
    "\n",
    "\n",
    "    if domains_set == 'df':\n",
    "        mask = (y != class_idx) & (k < num_df_domains) & (fs == 0)\n",
    "    elif domains_set == 'dp':\n",
    "        mask = (y != class_idx) & (k >= num_df_domains) & (fs == 0)\n",
    "    elif domains_set == 'dpfs':\n",
    "        mask = (y != class_idx) & (k >= num_df_domains) & (fs == 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid domains set: {domains_set}\")\n",
    "\n",
    "    # Apply initial mask\n",
    "    x, y, k = x[mask], y[mask], k[mask]\n",
    "\n",
    "    # Additional domain filtering if specified\n",
    "    if domain is not None:\n",
    "        domain_mask = (k == domain)\n",
    "        x, y, k = x[domain_mask], y[domain_mask], k[domain_mask]\n",
    "\n",
    "    assert len(x) > 0, f\"No data found\"\n",
    "    assert len(x) == len(y) == len(k), f\"Data length mismatch\"\n",
    "\n",
    "    return x, y, k\n",
    "\n",
    "\n",
    "\n",
    "def get_syn_data(dataset, syn_name, src_class, domain=None, fs=False):\n",
    "    # Load configurations\n",
    "    class_names = config[dataset]['class_names']\n",
    "\n",
    "    try:\n",
    "        with open(f'data/{dataset}_{syn_name}.pkl', 'rb') as f:\n",
    "            x, y, k = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Dataset files for {dataset} not found.\")\n",
    "\n",
    "    with open(f'data/{dataset}_{syn_name}_fs.pkl', 'rb') as f:\n",
    "        fs = pickle.load(f)\n",
    "\n",
    "    x, y, k = x[fs == 0], y[fs == 0], k[fs == 0]\n",
    "\n",
    "    if domain is not None:\n",
    "        domain_mask = (k == domain)\n",
    "        x, y, k = x[domain_mask], y[domain_mask], k[domain_mask]\n",
    "\n",
    "    assert len(x) > 0, f\"No data found\"\n",
    "    assert len(x) == len(y) == len(k), f\"Data length mismatch\"\n",
    "\n",
    "    return x, y, k\n",
    "\n",
    "\n",
    "\n",
    "class TSTRClassifier(nn.Module):\n",
    "    def __init__(self, num_timesteps=128, num_channels=3, num_classes=5):\n",
    "        super(TSTRClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.fc_shared = nn.Linear(num_timesteps * 8, 100)\n",
    "\n",
    "        self.fc_class = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc_shared(x))\n",
    "\n",
    "        # Final output for class prediction\n",
    "        class_outputs = self.fc_class(x)\n",
    "        return class_outputs\n",
    "\n",
    "\n",
    "\n",
    "def remap_labels(y):\n",
    "    label_map = {clss: i for i, clss in enumerate(np.unique(y))}\n",
    "    return np.array([label_map[clss] for clss in y])\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(x, y, shuffle=False):\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = remap_labels(y)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine dataset size and batch size\n",
    "    dataset_size = len(x)\n",
    "    if dataset_size <= 100:\n",
    "        batch_size = dataset_size\n",
    "    elif dataset_size <= 1000:\n",
    "        batch_size = 16\n",
    "    elif dataset_size <= 5000:\n",
    "        batch_size = 32\n",
    "    else:\n",
    "        batch_size = 64\n",
    "\n",
    "    dataset = TensorDataset(x, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def random_rotation_matrix():\n",
    "    \"\"\"Generate a random rotation matrix from a predefined set of quaternions.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Randomly generate a quaternion\n",
    "    q = np.random.rand(4)\n",
    "\n",
    "    # Convert quaternion to rotation matrix\n",
    "    q = torch.tensor(q, device=device, dtype=torch.float32)\n",
    "    q = q / torch.norm(q)  # Normalize quaternion\n",
    "    q0, q1, q2, q3 = q\n",
    "\n",
    "    R = torch.tensor([\n",
    "        [1 - 2*q2**2 - 2*q3**2, 2*q1*q2 - 2*q3*q0, 2*q1*q3 + 2*q2*q0],\n",
    "        [2*q1*q2 + 2*q3*q0, 1 - 2*q1**2 - 2*q3**2, 2*q2*q3 - 2*q1*q0],\n",
    "        [2*q1*q3 - 2*q2*q0, 2*q2*q3 + 2*q1*q0, 1 - 2*q1**2 - 2*q2**2]\n",
    "    ], device=device, dtype=torch.float32)\n",
    "\n",
    "    return R, q\n",
    "\n",
    "\n",
    "def augment_batch(x_real):\n",
    "    \"\"\"Apply random rotation to the batch of real time series.\"\"\"\n",
    "    min_val, max_val = -19.61, 19.61\n",
    "    x_real = x_real * (max_val - min_val) + min_val  # De-normalize\n",
    "    R, q = random_rotation_matrix()\n",
    "    x_real = torch.matmul(R, x_real)  # Apply rotation\n",
    "    x_real = (x_real - min_val) / (max_val - min_val)  # Re-normalize\n",
    "    return x_real, q\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted_labels.detach().cpu().numpy())\n",
    "            all_labels.extend(y_batch.detach().cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)        \n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    total_loss /= len(test_loader)\n",
    "\n",
    "    return accuracy, total_loss, f1\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=100, augment=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    accuracy_val = []\n",
    "    best_model_state = None\n",
    "    best_loss = np.inf\n",
    "    best_accuracy = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Set up linear learning rate decay\n",
    "    lambda_lr = lambda epoch: 1 - epoch / num_epochs\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            if augment:\n",
    "                x_batch, _ = augment_batch(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_loader)\n",
    "        loss_train.append(total_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        val_accuracy, val_loss, val_f1 = evaluate_model(model, val_loader)\n",
    "        if val_loss < best_loss:\n",
    "            best_epoch = epoch\n",
    "            best_accuracy = val_accuracy\n",
    "            best_f1 = val_f1\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "        loss_val.append(val_loss)\n",
    "        accuracy_val.append(val_accuracy)\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"\\tEpoch {epoch + 1}/{num_epochs} - Train loss: {total_loss:.4f} - Val accuracy: {val_accuracy:.4f} - Val loss: {val_loss:.4f} - Val F1: {val_f1:.4f} - LR: {current_lr:.2e}\")\n",
    "\n",
    "    print(f\"\\tBest epoch: {best_epoch + 1} - Best val accuracy: {best_accuracy:.4f} - Best val loss: {best_loss:.4f} - Best val F1: {best_f1:.4f}\\n\")\n",
    "\n",
    "    # Load best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_and_test(x_train, y_train, x_test, y_test, dataset, num_epochs=100, augment=False):\n",
    "    assert np.array_equal(np.unique(y_train), np.unique(y_test)), \"Training and test labels do not match\"\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "    test_loader = get_dataloader(x_test, y_test)\n",
    "\n",
    "    model = TSTRClassifier(num_timesteps=config[dataset]['num_timesteps'],\n",
    "                           num_channels=config[dataset]['num_channels'],\n",
    "                           num_classes=config[dataset]['num_classes']-1)\n",
    "    initial_lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs, augment)\n",
    "\n",
    "    test_accuracy, test_loss, test_f1 = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "    return test_accuracy, test_loss, test_f1\n",
    "\n",
    "\n",
    "\n",
    "def train_only(x_train, y_train, dataset, num_epochs=100, augment=False):\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "\n",
    "    model = TSTRClassifier(num_timesteps=config[dataset]['num_timesteps'],\n",
    "                           num_channels=config[dataset]['num_channels'],\n",
    "                           num_classes=config[dataset]['num_classes']-1)\n",
    "    initial_lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs, augment=augment)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "\n",
    "def train_classifier_cv(x_train, y_train, dataset, num_epochs=100):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    accs = []\n",
    "    losses = []\n",
    "    f1s = []\n",
    "\n",
    "    for train_index, test_index in skf.split(x_train, y_train):\n",
    "        x_train_fold, x_test_fold = x_train[train_index], x_train[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        acc, loss, f1 = train_and_test(x_train_fold, y_train_fold, x_test_fold, y_test_fold, dataset, num_epochs)\n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "        f1s.append(f1)\n",
    "    return np.mean(accs), np.mean(losses), np.mean(f1s)\n",
    "\n",
    "\n",
    "\n",
    "def fine_tune(model, x_train, y_train, num_epochs=100):\n",
    "    # Freeze feature extraction layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv' in name or 'bn' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "\n",
    "    initial_lr = 0.00001\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "\n",
    "def save_scores(source, domain, accuracy, loss, f1, name, dataset):\n",
    "    results_dir = 'results'\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Path to the CSV file\n",
    "    file_path = os.path.join(results_dir, f'{dataset}_{name}.csv')\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    # Open the file in append mode if it exists, or write mode if it doesn't\n",
    "    with open(file_path, mode='a' if file_exists else 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # If the file does not exist, write the header\n",
    "        if not file_exists:\n",
    "            writer.writerow(['source', 'domain', 'accuracy', 'loss', 'f1'])\n",
    "        # Write the data rows\n",
    "        writer.writerow([source, domain, accuracy, loss, f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRAlLTPumD5Z"
   },
   "source": [
    "# TSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732554888730,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "XrDIDrL9CPtn"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Dp(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            if domain == 74:\n",
    "              continue\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train and evaluate via cross-validation on Dp data\n",
    "            print('Training and evaluating on Dp data via cross-validation...')\n",
    "            acc, loss, f1 = train_classifier_cv(x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Dp', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Dp('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732554888730,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "M7S9IU2VVAmx"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Dpc(dataset):\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Dpc('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 594363,
     "status": "ok",
     "timestamp": 1732555483088,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "zhTuwDtYCPto",
    "outputId": "ff22e966-8ff0-4343-efdb-add2c84ba7e5"
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "def compute_TSTR_Df(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "\n",
    "        accs_run = []\n",
    "        f1s_run = []\n",
    "\n",
    "        for src_class in config[dataset]['class_names']:\n",
    "            if src_class != 'WAL':\n",
    "                continue\n",
    "\n",
    "            print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "            # Load Df data\n",
    "            x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "            print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "            # Train on Df data\n",
    "            print('Training on Df data...')\n",
    "            df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "            for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "                print(f\"Domain: {domain}\")\n",
    "\n",
    "                # Load Dp data\n",
    "                x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "                print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "                # Evaluate on Dp data\n",
    "                acc, loss, f1 = evaluate_model(df_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "                save_scores(src_class, domain, acc, loss, f1, 'Df', dataset)\n",
    "                accs_run.append(acc)\n",
    "                f1s_run.append(f1)\n",
    "                print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "        print(f\"Mean accuracy for run {i}: {np.mean(accs_run):.4f}\")\n",
    "        print(f\"Mean F1 for run {i}: {np.mean(f1s_run):.4f}\\n\")\n",
    "        accs.append(np.mean(accs_run))\n",
    "        f1s.append(np.mean(f1s_run))\n",
    "\n",
    "    print(f\"Mean accuracy over {n_runs} runs: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1 over {n_runs} runs: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687992,
     "status": "ok",
     "timestamp": 1732556171075,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "6X3sBMvtdTCl",
    "outputId": "47da434b-de61-462a-83a1-7e48c0a2ed8f"
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "def compute_TSTR_Df_aug(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "\n",
    "        accs_run = []\n",
    "        f1s_run = []\n",
    "\n",
    "        for src_class in config[dataset]['class_names']:\n",
    "            if src_class != 'WAL':\n",
    "                continue\n",
    "\n",
    "            print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "            # Load Df data\n",
    "            x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "            print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "            # Train on Df data\n",
    "            print('Training on Df data...')\n",
    "            df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs, augment=True)\n",
    "\n",
    "            for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "                print(f\"Domain: {domain}\")\n",
    "\n",
    "                # Load Dp data\n",
    "                x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "                print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "                # Evaluate on Dp data\n",
    "                acc, loss, f1 = evaluate_model(df_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "                save_scores(src_class, domain, acc, loss, f1, 'Df_aug', dataset)\n",
    "                accs_run.append(acc)\n",
    "                f1s_run.append(f1)\n",
    "                print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "        print(f\"Mean accuracy for run {i}: {np.mean(accs_run):.4f}\")\n",
    "        print(f\"Mean F1 for run {i}: {np.mean(f1s_run):.4f}\\n\")\n",
    "        accs.append(np.mean(accs_run))\n",
    "        f1s.append(np.mean(f1s_run))\n",
    "\n",
    "    print(f\"Mean accuracy over {n_runs} runs: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1 over {n_runs} runs: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df_aug('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 601055,
     "status": "ok",
     "timestamp": 1732556772105,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "aDSS4eIiUHeq",
    "outputId": "21d64766-f2f8-4678-b431-8d0607953e85"
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "def compute_TSTR_Df_rot(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "\n",
    "        accs_run = []\n",
    "        f1s_run = []\n",
    "\n",
    "        for src_class in config[dataset]['class_names']:\n",
    "            if src_class != 'WAL':\n",
    "                continue\n",
    "\n",
    "            print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "            # Load Df data\n",
    "            x_df, y_df, k_df = get_data(dataset, 'df', src_class, rot=True)\n",
    "            print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "            # Train on Df data\n",
    "            print('Training on Df data...')\n",
    "            df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "            for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "                print(f\"Domain: {domain}\")\n",
    "\n",
    "                # Load Dp data\n",
    "                x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "                print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "                # Evaluate on Dp data\n",
    "                acc, loss, f1 = evaluate_model(df_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "                save_scores(src_class, domain, acc, loss, f1, 'Df_rot', dataset)\n",
    "                accs_run.append(acc)\n",
    "                f1s_run.append(f1)\n",
    "                print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "        print(f\"Mean accuracy for run {i}: {np.mean(accs_run):.4f}\")\n",
    "        print(f\"Mean F1 for run {i}: {np.mean(f1s_run):.4f}\\n\")\n",
    "        accs.append(np.mean(accs_run))\n",
    "        f1s.append(np.mean(f1s_run))\n",
    "\n",
    "    print(f\"Mean accuracy over {n_runs} runs: {np.mean(accs):.4f} +- {np.std(accs):.4f}\")\n",
    "    print(f\"Mean F1 over {n_runs} runs: {np.mean(f1s):.4f} +- {np.std(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df_rot('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 444985,
     "status": "ok",
     "timestamp": 1732557217083,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "SmGQm1ZFCPto",
    "outputId": "635f030f-2f1a-406c-adec-926906dd999c"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Syn(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss, f1 = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TSTR_Syn_aug(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss, f1 = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs, augment=True)\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Syn_aug('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1732557217083,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "KKvjK_PAMxLm"
   },
   "outputs": [],
   "source": [
    "# def compute_TSTR_Syn_all(dataset):\n",
    "#     accs = []\n",
    "\n",
    "#     for src_class in config[dataset]['class_names']:\n",
    "#         if src_class != 'WAL':\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "#         # Load synthetic data\n",
    "#         x_syn, y_syn, k_syn = get_syn_data(dataset, syn_name, src_class)\n",
    "#         print(f'x_syn.shape: {x_syn.shape} | np.unique(y_syn): {np.unique(y_syn)}')\n",
    "\n",
    "#         # Load Dp data\n",
    "#         x_dp, y_dp, k_dp = get_data(dataset, 'dp', src_class)\n",
    "#         print(f'x_dp.shape: {x_dp.shape} | np.unique(y_dp): {np.unique(y_dp)}\\n')\n",
    "\n",
    "#         # Train on synthetic data and evaluate on Dp data\n",
    "#         print('Training on synthetic data...')\n",
    "#         acc, loss = train_and_test(x_syn, y_syn, x_dp, y_dp, dataset, num_epochs=num_epochs)\n",
    "#         save_scores(src_class, 100, acc, loss, 'Syn_all', dataset)\n",
    "#         accs.append(acc)\n",
    "#         print(f'Source class: {src_class} | Domain: All | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "#     print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Syn_all('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342631,
     "status": "ok",
     "timestamp": 1732557559707,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "LA71th1bCPto",
    "outputId": "33bfdb80-9301-4087-f41f-0b2d41777034"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Df_Syn(dataset):\n",
    "    accs = []\n",
    "    f1s = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=num_epochs)\n",
    "            acc, loss, f1 = evaluate_model(df_syn_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, f1, 'Df_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f} | F1: {f1:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(f1s):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1732557559973,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "WrQyLH7NMxLn"
   },
   "outputs": [],
   "source": [
    "# def compute_TSTR_Df_Syn_all(dataset):\n",
    "#     accs = []\n",
    "\n",
    "#     for src_class in config[dataset]['class_names']:\n",
    "#         if src_class != 'WAL':\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "#         # Load Df data\n",
    "#         x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "#         print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "#         # Train on Df data\n",
    "#         print('Training on Df data...')\n",
    "#         df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "#         # Load synthetic data\n",
    "#         x_syn, y_syn, k_syn = get_syn_data(dataset, syn_name, src_class)\n",
    "#         print(f'x_syn.shape: {x_syn.shape} | np.unique(y_syn): {np.unique(y_syn)}')\n",
    "\n",
    "#         # Load Dp data\n",
    "#         x_dp, y_dp, k_dp = get_data(dataset, 'dp', src_class)\n",
    "#         print(f'x_dp.shape: {x_dp.shape} | np.unique(y_dp): {np.unique(y_dp)}\\n')\n",
    "\n",
    "#         # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "#         print('Fine-tuning on synthetic data...')\n",
    "#         df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn, y_syn, num_epochs=num_epochs)\n",
    "#         acc, loss = evaluate_model(df_syn_model, get_dataloader(x_dp, y_dp))\n",
    "#         save_scores(src_class, 100, acc, loss, 'Df_Syn_all', dataset)\n",
    "#         accs.append(acc)\n",
    "#         print(f'Source class: {src_class} | Domain: All | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "#     print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Df_Syn_all('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLSuTddOmD5c"
   },
   "source": [
    "# TSTRFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZqvTTwNCPto"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Dpfs data and test on Dp data\n",
    "            print('Training on Dpfs data...')\n",
    "            acc, loss = train_and_test(x_dpfs_dom, y_dpfs_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOqZB6ARCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on Dpfs data and test on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            df_dpfs_model = fine_tune(copy.deepcopy(df_model), x_dpfs_dom, y_dpfs_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(df_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA89psVZg5yb"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38LHCQjRCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(df_syn_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueNEmwhVCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Syn_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data\n",
    "            print('Training on synthetic data...')\n",
    "            syn_model = train_only(x_syn_dom, y_syn_dom, dataset, num_epochs=num_epochs)\n",
    "\n",
    "            # Fine-tune synthetic model on Dpfs data and evaluate on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            syn_dpfs_model = fine_tune(copy.deepcopy(syn_model), x_dpfs_dom, y_dpfs_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(syn_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Syn_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Syn_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z0xuuhhCPtp"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Syn_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=num_epochs)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=num_epochs)\n",
    "\n",
    "            # Fine-tune Df-syn model on Dpfs data and evaluate on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            df_syn_dpfs_model = fine_tune(copy.deepcopy(df_syn_model), x_dpfs_dom, y_dpfs_dom, num_epochs=num_epochs)\n",
    "            acc, loss = evaluate_model(df_syn_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Syn_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Syn_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpB3ecNGmD5f"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_plus_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Join Df and Dpfs data\n",
    "            x_df_dpfs = np.concatenate([x_df, x_dpfs_dom], axis=0)\n",
    "            y_df_dpfs = np.concatenate([y_df, y_dpfs_dom], axis=0)\n",
    "            k_df_dpfs = np.concatenate([k_df, k_dpfs_dom], axis=0)\n",
    "            print(f'x_df_dpfs.shape: {x_df_dpfs.shape} | np.unique(y_df_dpfs): {np.unique(y_df_dpfs)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Df plus Dpfs data and test on Dp data\n",
    "            print('Training on Df plus Dpfs data...')\n",
    "            acc, loss = train_and_test(x_df_dpfs, y_df_dpfs, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_plus_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_plus_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYl0a9ZJmD5f"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_plus_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Join Df and synthetic data\n",
    "            x_df_syn = np.concatenate([x_df, x_syn_dom], axis=0)\n",
    "            y_df_syn = np.concatenate([y_df, y_syn_dom], axis=0)\n",
    "            k_df_syn = np.concatenate([k_df, k_syn_dom], axis=0)\n",
    "            print(f'x_df_syn.shape: {x_df_syn.shape} | np.unique(y_df_syn): {np.unique(y_df_syn)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Df plus synthetic data and test on Dp data\n",
    "            print('Training on Df plus synthetic data...')\n",
    "            acc, loss = train_and_test(x_df, y_df, x_dp_dom, y_dp_dom, dataset, num_epochs=num_epochs)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_plus_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_plus_Syn('realworld_mobiact')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
