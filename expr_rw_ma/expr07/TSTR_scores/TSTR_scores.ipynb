{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18869,
     "status": "ok",
     "timestamp": 1731517769896,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "I0_1KsKpCPtk",
    "outputId": "68652ccc-bee5-4a43-a34e-d9df51c9ed49"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd drive/MyDrive/ST/stargan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLVVBGiQmD5W"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5394,
     "status": "ok",
     "timestamp": 1731517775285,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "yh05UGFkCPtm"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import copy\n",
    "\n",
    "seed = 2710\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731517775286,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "UJN2bKbaCPtm"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'realworld': {\n",
    "        'dataset_name': 'realworld',\n",
    "        'num_df_domains': 10,\n",
    "        'num_dp_domains': 5,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['WAL', 'RUN', 'CLD', 'CLU'],\n",
    "        'num_timesteps': 128,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 4,\n",
    "    },\n",
    "    'cwru': {\n",
    "        'dataset_name': 'cwru_256_3ch_5cl',\n",
    "        'num_df_domains': 4,\n",
    "        'num_dp_domains': 4,\n",
    "        'num_classes': 5,\n",
    "        'class_names': ['IR', 'Ball', 'OR_centred', 'OR_orthogonal', 'OR_opposite'],\n",
    "        'num_timesteps': 256,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 5,\n",
    "    },\n",
    "    'realworld_mobiact': {\n",
    "        'dataset_name': 'realworld_mobiact',\n",
    "        'num_df_domains': 15,\n",
    "        'num_dp_domains': 61,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['WAL', 'RUN', 'CLD', 'CLU'],\n",
    "        'num_timesteps': 128,\n",
    "        'num_channels': 3,\n",
    "        'num_classes': 4,\n",
    "    }\n",
    "}\n",
    "\n",
    "syn_name = 'rwma07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731517775287,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "XwSwaF-LCPtn"
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, domains_set, src_class, domain=None):\n",
    "    # Load configurations\n",
    "    dataset_name = config[dataset]['dataset_name']\n",
    "    class_idx = config[dataset]['class_names'].index(src_class)\n",
    "    num_df_domains = config[dataset]['num_df_domains']\n",
    "\n",
    "    try:\n",
    "        with open(f'data/{dataset_name}.pkl', 'rb') as f:\n",
    "            x, y, k = pickle.load(f)\n",
    "        with open(f'data/{dataset_name}_fs.pkl', 'rb') as f:\n",
    "            fs = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Dataset files for {dataset} not found.\")\n",
    "\n",
    "\n",
    "    if domains_set == 'df':\n",
    "        mask = (y != class_idx) & (k < num_df_domains) & (fs == 0)\n",
    "    elif domains_set == 'dp':\n",
    "        mask = (y != class_idx) & (k >= num_df_domains) & (fs == 0)\n",
    "    elif domains_set == 'dpfs':\n",
    "        mask = (y != class_idx) & (k >= num_df_domains) & (fs == 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid domains set: {domains_set}\")\n",
    "\n",
    "    # Apply initial mask\n",
    "    x, y, k = x[mask], y[mask], k[mask]\n",
    "\n",
    "    # Additional domain filtering if specified\n",
    "    if domain is not None:\n",
    "        domain_mask = (k == domain)\n",
    "        x, y, k = x[domain_mask], y[domain_mask], k[domain_mask]\n",
    "\n",
    "    assert len(x) > 0, f\"No data found\"\n",
    "    assert len(x) == len(y) == len(k), f\"Data length mismatch\"\n",
    "\n",
    "    return x, y, k\n",
    "\n",
    "\n",
    "\n",
    "def get_syn_data(dataset, syn_name, src_class, domain=None, fs=False):\n",
    "    # Load configurations\n",
    "    class_names = config[dataset]['class_names']\n",
    "\n",
    "    try:\n",
    "        with open(f'data/{dataset}_{syn_name}.pkl', 'rb') as f:\n",
    "            x, y, k = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Dataset files for {dataset} not found.\")\n",
    "\n",
    "    with open(f'data/{dataset}_fs_{syn_name}.pkl', 'rb') as f:\n",
    "        fs = pickle.load(f)\n",
    "\n",
    "    x, y, k = x[fs == 0], y[fs == 0], k[fs == 0]\n",
    "\n",
    "    if domain is not None:\n",
    "        domain_mask = (k == domain)\n",
    "        x, y, k = x[domain_mask], y[domain_mask], k[domain_mask]\n",
    "\n",
    "    assert len(x) > 0, f\"No data found\"\n",
    "    assert len(x) == len(y) == len(k), f\"Data length mismatch\"\n",
    "\n",
    "    return x, y, k\n",
    "\n",
    "\n",
    "\n",
    "class TSTRClassifier(nn.Module):\n",
    "    def __init__(self, num_timesteps=128, num_channels=3, num_classes=5):\n",
    "        super(TSTRClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.fc_shared = nn.Linear(num_timesteps * 8, 100)\n",
    "\n",
    "        self.fc_class = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc_shared(x))\n",
    "\n",
    "        # Final output for class prediction\n",
    "        class_outputs = self.fc_class(x)\n",
    "        return class_outputs\n",
    "\n",
    "\n",
    "\n",
    "def remap_labels(y):\n",
    "    label_map = {clss: i for i, clss in enumerate(np.unique(y))}\n",
    "    return np.array([label_map[clss] for clss in y])\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(x, y, shuffle=False):\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = remap_labels(y)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine dataset size and batch size\n",
    "    dataset_size = len(x)\n",
    "    if dataset_size <= 100:\n",
    "        batch_size = dataset_size\n",
    "    elif dataset_size <= 1000:\n",
    "        batch_size = 16\n",
    "    elif dataset_size <= 5000:\n",
    "        batch_size = 32\n",
    "    else:\n",
    "        batch_size = 64\n",
    "\n",
    "    dataset = TensorDataset(x, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted_labels == y_batch).sum().item()\n",
    "            total_predictions += len(y_batch)\n",
    "\n",
    "    total_loss /= len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy, total_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    accuracy_val = []\n",
    "    best_model_state = None\n",
    "    best_loss = np.inf\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Set up linear learning rate decay\n",
    "    lambda_lr = lambda epoch: 1 - epoch / num_epochs\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_loader)\n",
    "        loss_train.append(total_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        val_accuracy, val_loss = evaluate_model(model, val_loader)\n",
    "        if val_loss < best_loss:\n",
    "            best_epoch = epoch\n",
    "            best_accuracy = val_accuracy\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "        loss_val.append(val_loss)\n",
    "        accuracy_val.append(val_accuracy)\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"\\tEpoch {epoch + 1}/{num_epochs} - Train loss: {total_loss:.4f} - Val accuracy: {val_accuracy:.4f} - Val loss: {val_loss:.4f} - LR: {current_lr:.2e}\")\n",
    "\n",
    "    print(f\"\\tBest epoch: {best_epoch + 1} - Best val accuracy: {best_accuracy:.4f} - Best val loss: {best_loss:.4f}\\n\")\n",
    "\n",
    "    # Load best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_and_test(x_train, y_train, x_test, y_test, dataset, num_epochs=100):\n",
    "    assert np.array_equal(np.unique(y_train), np.unique(y_test)), \"Training and test labels do not match\"\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "    test_loader = get_dataloader(x_test, y_test)\n",
    "\n",
    "    model = TSTRClassifier(num_timesteps=config[dataset]['num_timesteps'],\n",
    "                           num_channels=config[dataset]['num_channels'],\n",
    "                           num_classes=config[dataset]['num_classes']-1)\n",
    "    initial_lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs)\n",
    "\n",
    "    test_accuracy, test_loss = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "    return test_accuracy, test_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_only(x_train, y_train, dataset, num_epochs=100):\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "\n",
    "    model = TSTRClassifier(num_timesteps=config[dataset]['num_timesteps'],\n",
    "                           num_channels=config[dataset]['num_channels'],\n",
    "                           num_classes=config[dataset]['num_classes']-1)\n",
    "    initial_lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "\n",
    "def train_classifier_cv(x_train, y_train, dataset, num_epochs=100):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    accs = []\n",
    "    losses = []\n",
    "\n",
    "    for train_index, test_index in skf.split(x_train, y_train):\n",
    "        x_train_fold, x_test_fold = x_train[train_index], x_train[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        acc, loss = train_and_test(x_train_fold, y_train_fold, x_test_fold, y_test_fold, dataset, num_epochs)\n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.mean(accs), np.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "def fine_tune(model, x_train, y_train, num_epochs=100):\n",
    "    # Freeze feature extraction layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv' in name or 'bn' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, shuffle=True, random_state=seed)\n",
    "\n",
    "    train_loader = get_dataloader(x_tr, y_tr, shuffle=True)\n",
    "    val_loader = get_dataloader(x_val, y_val)\n",
    "\n",
    "    initial_lr = 0.00001\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=initial_lr)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, num_epochs)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "\n",
    "def save_scores(source, domain, accuracy, loss, name, dataset):\n",
    "    results_dir = 'results'\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Path to the CSV file\n",
    "    file_path = os.path.join(results_dir, f'{dataset}_{name}.csv')\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    # Open the file in append mode if it exists, or write mode if it doesn't\n",
    "    with open(file_path, mode='a' if file_exists else 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # If the file does not exist, write the header\n",
    "        if not file_exists:\n",
    "            writer.writerow(['source', 'domain', 'accuracy', 'loss'])\n",
    "        # Write the data rows\n",
    "        writer.writerow([source, domain, accuracy, loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRAlLTPumD5Z"
   },
   "source": [
    "# TSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrDIDrL9CPtn"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Dp(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train and evaluate via cross-validation on Dp data\n",
    "            print('Training and evaluating on Dp data via cross-validation...')\n",
    "            acc, loss = train_classifier_cv(x_dp_dom, y_dp_dom, dataset, num_epochs=100)\n",
    "            save_scores(src_class, domain, acc, loss, 'Dp', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Dp('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7S9IU2VVAmx"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Dpc(dataset):\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "# compute_TSTR_Dpc('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zhTuwDtYCPto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source class: WAL\n",
      "\n",
      "x_df.shape: (7958, 3, 128) | np.unique(y_df): [1 2 3]\n",
      "\n",
      "Training on Df data...\n",
      "\tEpoch 10/100 - Train loss: 0.0383 - Val accuracy: 0.9812 - Val loss: 0.0518 - LR: 9.00e-05\n",
      "\tEpoch 20/100 - Train loss: 0.0111 - Val accuracy: 0.9818 - Val loss: 0.0443 - LR: 8.00e-05\n",
      "\tEpoch 30/100 - Train loss: 0.0058 - Val accuracy: 0.9843 - Val loss: 0.0425 - LR: 7.00e-05\n",
      "\tEpoch 40/100 - Train loss: 0.0020 - Val accuracy: 0.9856 - Val loss: 0.0426 - LR: 6.00e-05\n",
      "\tEpoch 50/100 - Train loss: 0.0012 - Val accuracy: 0.9881 - Val loss: 0.0440 - LR: 5.00e-05\n",
      "\tEpoch 60/100 - Train loss: 0.0013 - Val accuracy: 0.9868 - Val loss: 0.0476 - LR: 4.00e-05\n",
      "\tEpoch 70/100 - Train loss: 0.0004 - Val accuracy: 0.9874 - Val loss: 0.0454 - LR: 3.00e-05\n",
      "\tEpoch 80/100 - Train loss: 0.0003 - Val accuracy: 0.9874 - Val loss: 0.0561 - LR: 2.00e-05\n",
      "\tEpoch 90/100 - Train loss: 0.0015 - Val accuracy: 0.9874 - Val loss: 0.0597 - LR: 1.00e-05\n",
      "\tEpoch 100/100 - Train loss: 0.0002 - Val accuracy: 0.9868 - Val loss: 0.0529 - LR: 0.00e+00\n",
      "\tBest epoch: 28 - Best val accuracy: 0.9843 - Best val loss: 0.0396\n",
      "\n",
      "Domain: 15\n",
      "x_dp_dom.shape: (50, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 15 | Accuracy: 0.2400 | Loss: 12.4001\n",
      "\n",
      "Domain: 16\n",
      "x_dp_dom.shape: (54, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 16 | Accuracy: 0.6667 | Loss: 1.4607\n",
      "\n",
      "Domain: 17\n",
      "x_dp_dom.shape: (53, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 17 | Accuracy: 0.7736 | Loss: 1.8685\n",
      "\n",
      "Domain: 18\n",
      "x_dp_dom.shape: (54, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 18 | Accuracy: 0.7222 | Loss: 3.4425\n",
      "\n",
      "Domain: 19\n",
      "x_dp_dom.shape: (54, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 19 | Accuracy: 0.7593 | Loss: 2.1205\n",
      "\n",
      "Domain: 20\n",
      "x_dp_dom.shape: (51, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 20 | Accuracy: 0.7451 | Loss: 1.0905\n",
      "\n",
      "Domain: 21\n",
      "x_dp_dom.shape: (47, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 21 | Accuracy: 0.7660 | Loss: 1.4198\n",
      "\n",
      "Domain: 22\n",
      "x_dp_dom.shape: (54, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 22 | Accuracy: 0.4815 | Loss: 4.1884\n",
      "\n",
      "Domain: 23\n",
      "x_dp_dom.shape: (52, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 23 | Accuracy: 0.7692 | Loss: 3.2354\n",
      "\n",
      "Domain: 24\n",
      "x_dp_dom.shape: (51, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 24 | Accuracy: 0.6275 | Loss: 1.3604\n",
      "\n",
      "Domain: 25\n",
      "x_dp_dom.shape: (54, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 25 | Accuracy: 0.7593 | Loss: 2.5939\n",
      "\n",
      "Domain: 26\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 26 | Accuracy: 0.6905 | Loss: 0.7904\n",
      "\n",
      "Domain: 27\n",
      "x_dp_dom.shape: (54, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 27 | Accuracy: 0.7593 | Loss: 4.8378\n",
      "\n",
      "Domain: 28\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 28 | Accuracy: 0.8095 | Loss: 0.8641\n",
      "\n",
      "Domain: 29\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 29 | Accuracy: 0.5714 | Loss: 2.9229\n",
      "\n",
      "Domain: 30\n",
      "x_dp_dom.shape: (50, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 30 | Accuracy: 0.4200 | Loss: 4.9856\n",
      "\n",
      "Domain: 31\n",
      "x_dp_dom.shape: (43, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 31 | Accuracy: 0.8372 | Loss: 1.5356\n",
      "\n",
      "Domain: 32\n",
      "x_dp_dom.shape: (52, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 32 | Accuracy: 0.6731 | Loss: 1.9860\n",
      "\n",
      "Domain: 33\n",
      "x_dp_dom.shape: (44, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 33 | Accuracy: 0.8182 | Loss: 1.3429\n",
      "\n",
      "Domain: 34\n",
      "x_dp_dom.shape: (49, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 34 | Accuracy: 0.5714 | Loss: 2.7400\n",
      "\n",
      "Domain: 35\n",
      "x_dp_dom.shape: (48, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 35 | Accuracy: 0.9167 | Loss: 0.2076\n",
      "\n",
      "Domain: 36\n",
      "x_dp_dom.shape: (43, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 36 | Accuracy: 0.8140 | Loss: 1.2904\n",
      "\n",
      "Domain: 37\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 37 | Accuracy: 0.7857 | Loss: 0.7716\n",
      "\n",
      "Domain: 38\n",
      "x_dp_dom.shape: (47, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 38 | Accuracy: 0.4894 | Loss: 6.3035\n",
      "\n",
      "Domain: 39\n",
      "x_dp_dom.shape: (54, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 39 | Accuracy: 0.4074 | Loss: 12.3055\n",
      "\n",
      "Domain: 40\n",
      "x_dp_dom.shape: (44, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 40 | Accuracy: 0.6818 | Loss: 1.4864\n",
      "\n",
      "Domain: 41\n",
      "x_dp_dom.shape: (45, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 41 | Accuracy: 0.6667 | Loss: 6.8511\n",
      "\n",
      "Domain: 42\n",
      "x_dp_dom.shape: (46, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 42 | Accuracy: 0.8478 | Loss: 2.7210\n",
      "\n",
      "Domain: 43\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 43 | Accuracy: 0.5476 | Loss: 7.3175\n",
      "\n",
      "Domain: 44\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 44 | Accuracy: 0.8333 | Loss: 1.6076\n",
      "\n",
      "Domain: 45\n",
      "x_dp_dom.shape: (52, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 45 | Accuracy: 0.7500 | Loss: 0.9727\n",
      "\n",
      "Domain: 46\n",
      "x_dp_dom.shape: (48, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 46 | Accuracy: 0.8125 | Loss: 1.8117\n",
      "\n",
      "Domain: 47\n",
      "x_dp_dom.shape: (44, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 47 | Accuracy: 0.8409 | Loss: 0.9387\n",
      "\n",
      "Domain: 48\n",
      "x_dp_dom.shape: (48, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 48 | Accuracy: 0.7083 | Loss: 1.8849\n",
      "\n",
      "Domain: 49\n",
      "x_dp_dom.shape: (53, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 49 | Accuracy: 0.8113 | Loss: 0.8022\n",
      "\n",
      "Domain: 50\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 50 | Accuracy: 0.6667 | Loss: 1.5532\n",
      "\n",
      "Domain: 51\n",
      "x_dp_dom.shape: (46, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 51 | Accuracy: 0.6522 | Loss: 2.3830\n",
      "\n",
      "Domain: 52\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 52 | Accuracy: 0.8571 | Loss: 0.9494\n",
      "\n",
      "Domain: 53\n",
      "x_dp_dom.shape: (48, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 53 | Accuracy: 0.7708 | Loss: 1.0164\n",
      "\n",
      "Domain: 54\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 54 | Accuracy: 0.7619 | Loss: 1.2987\n",
      "\n",
      "Domain: 55\n",
      "x_dp_dom.shape: (49, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 55 | Accuracy: 0.9388 | Loss: 0.1490\n",
      "\n",
      "Domain: 56\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 56 | Accuracy: 0.8095 | Loss: 1.0295\n",
      "\n",
      "Domain: 57\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 57 | Accuracy: 0.6190 | Loss: 1.7903\n",
      "\n",
      "Domain: 58\n",
      "x_dp_dom.shape: (38, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 58 | Accuracy: 0.8684 | Loss: 1.9410\n",
      "\n",
      "Domain: 59\n",
      "x_dp_dom.shape: (43, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 59 | Accuracy: 0.8140 | Loss: 0.5880\n",
      "\n",
      "Domain: 60\n",
      "x_dp_dom.shape: (47, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 60 | Accuracy: 0.4255 | Loss: 14.1755\n",
      "\n",
      "Domain: 61\n",
      "x_dp_dom.shape: (47, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 61 | Accuracy: 0.6596 | Loss: 1.6770\n",
      "\n",
      "Domain: 62\n",
      "x_dp_dom.shape: (47, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 62 | Accuracy: 0.8511 | Loss: 0.3686\n",
      "\n",
      "Domain: 63\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 63 | Accuracy: 0.7857 | Loss: 1.2791\n",
      "\n",
      "Domain: 64\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 64 | Accuracy: 0.8571 | Loss: 0.5658\n",
      "\n",
      "Domain: 65\n",
      "x_dp_dom.shape: (50, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 65 | Accuracy: 0.3200 | Loss: 9.7952\n",
      "\n",
      "Domain: 66\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 66 | Accuracy: 0.1190 | Loss: 13.9478\n",
      "\n",
      "Domain: 67\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 67 | Accuracy: 0.5952 | Loss: 2.7488\n",
      "\n",
      "Domain: 68\n",
      "x_dp_dom.shape: (43, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 68 | Accuracy: 0.7209 | Loss: 1.0942\n",
      "\n",
      "Domain: 69\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 69 | Accuracy: 0.2381 | Loss: 3.5712\n",
      "\n",
      "Domain: 70\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 70 | Accuracy: 0.7619 | Loss: 3.0238\n",
      "\n",
      "Domain: 71\n",
      "x_dp_dom.shape: (44, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 71 | Accuracy: 0.8636 | Loss: 0.9080\n",
      "\n",
      "Domain: 72\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 72 | Accuracy: 0.8571 | Loss: 0.2930\n",
      "\n",
      "Domain: 73\n",
      "x_dp_dom.shape: (41, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 73 | Accuracy: 0.1707 | Loss: 14.0724\n",
      "\n",
      "Domain: 74\n",
      "x_dp_dom.shape: (35, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 74 | Accuracy: 0.7429 | Loss: 2.8309\n",
      "\n",
      "Domain: 75\n",
      "x_dp_dom.shape: (42, 3, 128) | np.unique(y_dp_dom): [1 2 3]\n",
      "\n",
      "Source class: WAL | Domain: 75 | Accuracy: 0.7381 | Loss: 1.0438\n",
      "\n",
      "Mean accuracy: 0.6859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_TSTR_Df(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=100)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Evaluate on Dp data\n",
    "            acc, loss = evaluate_model(df_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'Df', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmGQm1ZFCPto"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=100)\n",
    "            save_scores(src_class, domain, acc, loss, 'Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TSTR_Syn_all(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load synthetic data\n",
    "        x_syn, y_syn, k_syn = get_syn_data(dataset, syn_name, src_class)\n",
    "        print(f'x_syn.shape: {x_syn.shape} | np.unique(y_syn): {np.unique(y_syn)}')\n",
    "\n",
    "        # Load Dp data\n",
    "        x_dp, y_dp, k_dp = get_data(dataset, 'dp', src_class)\n",
    "        print(f'x_dp.shape: {x_dp.shape} | np.unique(y_dp): {np.unique(y_dp)}\\n')\n",
    "\n",
    "        # Train on synthetic data and evaluate on Dp data\n",
    "        print('Training on synthetic data...')\n",
    "        acc, loss = train_and_test(x_syn, y_syn, x_dp, y_dp, dataset, num_epochs=100)\n",
    "        save_scores(src_class, 100, acc, loss, 'Syn_all', dataset)\n",
    "        accs.append(acc)\n",
    "        print(f'Source class: {src_class} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Syn_all('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LA71th1bCPto"
   },
   "outputs": [],
   "source": [
    "def compute_TSTR_Df_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=100)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=100)\n",
    "            acc, loss = evaluate_model(df_syn_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'Df_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTR_Df_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLSuTddOmD5c"
   },
   "source": [
    "# TSTRFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15410,
     "status": "ok",
     "timestamp": 1731517790691,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "wZqvTTwNCPto",
    "outputId": "57e307ee-e165-4f59-b334-1e768e7bc3fd"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Dpfs data and test on Dp data\n",
    "            print('Training on Dpfs data...')\n",
    "            acc, loss = train_and_test(x_dpfs_dom, y_dpfs_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=100)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70863,
     "status": "ok",
     "timestamp": 1731517861551,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "YOqZB6ARCPtp",
    "outputId": "e9bb42ae-2d55-4364-ccd5-4ca4dbefbce9"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=100)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on Dpfs data and test on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            df_dpfs_model = fine_tune(copy.deepcopy(df_model), x_dpfs_dom, y_dpfs_dom, num_epochs=100)\n",
    "            acc, loss = evaluate_model(df_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84019,
     "status": "ok",
     "timestamp": 1731517945564,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "oA89psVZg5yb",
    "outputId": "3d5c1ef2-461c-4813-9c3e-e47f744a036b"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data and evaluate on Dp data\n",
    "            print('Training on synthetic data...')\n",
    "            acc, loss = train_and_test(x_syn_dom, y_syn_dom, x_dp_dom, y_dp_dom, dataset, num_epochs=100)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115803,
     "status": "ok",
     "timestamp": 1731518061363,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "38LHCQjRCPtp",
    "outputId": "822fad62-ac82-4f27-cf8f-80c29dfe5b68"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=100)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data and evaluate on Dp data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=100)\n",
    "            acc, loss = evaluate_model(df_syn_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Syn('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84343,
     "status": "ok",
     "timestamp": 1731518145701,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "ueNEmwhVCPtp",
    "outputId": "5ee9d485-ce7f-4ed5-8aa4-2328836bc5f2"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Syn_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on synthetic data\n",
    "            print('Training on synthetic data...')\n",
    "            syn_model = train_only(x_syn_dom, y_syn_dom, dataset, num_epochs=100)\n",
    "\n",
    "            # Fine-tune synthetic model on Dpfs data and evaluate on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            syn_dpfs_model = fine_tune(copy.deepcopy(syn_model), x_dpfs_dom, y_dpfs_dom, num_epochs=100)\n",
    "            acc, loss = evaluate_model(syn_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Syn_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Syn_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119727,
     "status": "ok",
     "timestamp": 1731518265420,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "7z0xuuhhCPtp",
    "outputId": "ef508df7-2287-4b52-810b-87437bab6bd9"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_Syn_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        # Train on Df data\n",
    "        print('Training on Df data...')\n",
    "        df_model = train_only(x_df, y_df, dataset, num_epochs=100)\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Fine-tune Df model on synthetic data\n",
    "            print('Fine-tuning on synthetic data...')\n",
    "            df_syn_model = fine_tune(copy.deepcopy(df_model), x_syn_dom, y_syn_dom, num_epochs=100)\n",
    "\n",
    "            # Fine-tune Df-syn model on Dpfs data and evaluate on Dp data\n",
    "            print('Fine-tuning on Dpfs data...')\n",
    "            df_syn_dpfs_model = fine_tune(copy.deepcopy(df_syn_model), x_dpfs_dom, y_dpfs_dom, num_epochs=100)\n",
    "            acc, loss = evaluate_model(df_syn_dpfs_model, get_dataloader(x_dp_dom, y_dp_dom))\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_Syn_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_Syn_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307577,
     "status": "ok",
     "timestamp": 1731518572986,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "zpB3ecNGmD5f",
    "outputId": "a2981bb2-6a3a-44c1-d7b8-f12d008919aa"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_plus_Dpfs(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load Dpfs data\n",
    "            x_dpfs_dom, y_dpfs_dom, k_dpfs_dom = get_data(dataset, 'dpfs', src_class, domain)\n",
    "            print(f'x_dpfs_dom.shape: {x_dpfs_dom.shape} | np.unique(y_dpfs_dom): {np.unique(y_dpfs_dom)}')\n",
    "\n",
    "            # Join Df and Dpfs data\n",
    "            x_df_dpfs = np.concatenate([x_df, x_dpfs_dom], axis=0)\n",
    "            y_df_dpfs = np.concatenate([y_df, y_dpfs_dom], axis=0)\n",
    "            k_df_dpfs = np.concatenate([k_df, k_dpfs_dom], axis=0)\n",
    "            print(f'x_df_dpfs.shape: {x_df_dpfs.shape} | np.unique(y_df_dpfs): {np.unique(y_df_dpfs)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Df plus Dpfs data and test on Dp data\n",
    "            print('Training on Df plus Dpfs data...')\n",
    "            acc, loss = train_and_test(x_df_dpfs, y_df_dpfs, x_dp_dom, y_dp_dom, dataset, num_epochs=100)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_plus_Dpfs', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_plus_Dpfs('realworld_mobiact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303689,
     "status": "ok",
     "timestamp": 1731518876660,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "RYl0a9ZJmD5f",
    "outputId": "78997be5-7c3b-450a-fff7-fcfe302b8b7e"
   },
   "outputs": [],
   "source": [
    "def compute_TSTRFS_Df_plus_Syn(dataset):\n",
    "    accs = []\n",
    "\n",
    "    for src_class in config[dataset]['class_names']:\n",
    "        if src_class != 'WAL':\n",
    "            continue\n",
    "\n",
    "        print(f\"Source class: {src_class}\\n\")\n",
    "\n",
    "        # Load Df data\n",
    "        x_df, y_df, k_df = get_data(dataset, 'df', src_class)\n",
    "        print(f'x_df.shape: {x_df.shape} | np.unique(y_df): {np.unique(y_df)}\\n')\n",
    "\n",
    "        for domain in range(config[dataset]['num_df_domains'], config[dataset]['num_df_domains'] + config[dataset]['num_dp_domains']):\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            # Load synthetic data\n",
    "            x_syn_dom, y_syn_dom, k_syn_dom = get_syn_data(dataset, syn_name, src_class, domain)\n",
    "            print(f'x_syn_dom.shape: {x_syn_dom.shape} | np.unique(y_syn_dom): {np.unique(y_syn_dom)}')\n",
    "\n",
    "            # Join Df and synthetic data\n",
    "            x_df_syn = np.concatenate([x_df, x_syn_dom], axis=0)\n",
    "            y_df_syn = np.concatenate([y_df, y_syn_dom], axis=0)\n",
    "            k_df_syn = np.concatenate([k_df, k_syn_dom], axis=0)\n",
    "            print(f'x_df_syn.shape: {x_df_syn.shape} | np.unique(y_df_syn): {np.unique(y_df_syn)}')\n",
    "\n",
    "            # Load Dp data\n",
    "            x_dp_dom, y_dp_dom, k_dp_dom = get_data(dataset, 'dp', src_class, domain)\n",
    "            print(f'x_dp_dom.shape: {x_dp_dom.shape} | np.unique(y_dp_dom): {np.unique(y_dp_dom)}\\n')\n",
    "\n",
    "            # Train on Df plus synthetic data and test on Dp data\n",
    "            print('Training on Df plus synthetic data...')\n",
    "            acc, loss = train_and_test(x_df, y_df, x_dp_dom, y_dp_dom, dataset, num_epochs=100)\n",
    "            save_scores(src_class, domain, acc, loss, 'FS_Df_plus_Syn', dataset)\n",
    "            accs.append(acc)\n",
    "            print(f'Source class: {src_class} | Domain: {domain} | Accuracy: {acc:.4f} | Loss: {loss:.4f}\\n')\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accs):.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compute_TSTRFS_Df_plus_Syn('realworld_mobiact')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
