{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23666,
     "status": "ok",
     "timestamp": 1730886666184,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "aaerYod3jW-_",
    "outputId": "4f4e3ed4-e90a-4511-f9ae-735b8ec693ee"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd drive/MyDrive/ST/stargan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730886666185,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "oAA8pHAojW_A"
   },
   "outputs": [],
   "source": [
    "dataset = 'realworld' # 'realworld' or 'cwru' or 'realworld_mobiact'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL8NaAwojW_B"
   },
   "source": [
    "# Pre-trained Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnvaaxnSjW_C"
   },
   "source": [
    "## Domain Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6819,
     "status": "ok",
     "timestamp": 1730886673000,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "KFG4iT6SjW_D"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "seed = 2710\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class MultiBranchClassifier(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_domains=4, num_classes=5, num_timesteps=128):\n",
    "        super(MultiBranchClassifier, self).__init__()\n",
    "        # Shared layers for all branches\n",
    "        self.conv1 = nn.Conv1d(num_channels, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc_shared = nn.Linear(num_timesteps * 8, 100)\n",
    "\n",
    "        # Prepare class-specific branches as a single module with conditionally applied outputs\n",
    "        self.fc_class_branches = nn.Linear(100, 50 * num_classes)\n",
    "        self.fc_final = nn.Linear(50, num_domains)\n",
    "\n",
    "    def forward(self, x, class_ids):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc_shared(x))\n",
    "\n",
    "        # Process all class-specific branches simultaneously\n",
    "        class_branches = self.fc_class_branches(x).view(x.size(0), -1, 50)\n",
    "        class_outputs = class_branches[torch.arange(class_branches.size(0)), class_ids]\n",
    "\n",
    "        # Final class-specific output\n",
    "        final_outputs = self.fc_final(class_outputs.view(x.size(0), 50))\n",
    "        return final_outputs.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "def split_data(x, y, k, test_size=0.2, random_state=seed):\n",
    "    x_train, x_test, y_train, y_test, k_train, k_test = train_test_split(x, y, k,\n",
    "                                                                         test_size=test_size,\n",
    "                                                                         random_state=random_state,\n",
    "                                                                         stratify=k,\n",
    "                                                                         shuffle=True)\n",
    "    return x_train, x_test, y_train, y_test, k_train, k_test\n",
    "\n",
    "\n",
    "def setup_training(x_train, y_train, k_train, x_test, y_test, k_test, batch_size=64):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    k_train_tensor = torch.tensor(k_train, dtype=torch.long)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    k_test_tensor = torch.tensor(k_test, dtype=torch.long)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor, k_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor, k_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, k_batch in test_loader:\n",
    "            x_batch, y_batch, k_batch = x_batch.to(device), y_batch.to(device), k_batch.to(device)\n",
    "            outputs = model(x_batch, y_batch)\n",
    "            loss = F.cross_entropy(outputs, k_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Assuming outputs are logits and k_batch are the true labels\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted_labels == k_batch).sum().item()\n",
    "            total_predictions += k_batch.size(0)\n",
    "\n",
    "    total_loss /= len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return total_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, epochs=300, name='domain_classifier'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    accuracy_test = []\n",
    "    best_loss = np.inf\n",
    "\n",
    "    # Set up linear learning rate decay\n",
    "    lambda_lr = lambda epoch: 1 - epoch / epochs\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch, k_batch in train_loader:\n",
    "            x_batch, y_batch, k_batch = x_batch.to(device), y_batch.to(device), k_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch, y_batch)\n",
    "            loss = F.cross_entropy(outputs, k_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_loader)\n",
    "        loss_train.append(total_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, device=device)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "        loss_test.append(test_loss)\n",
    "        accuracy_test.append(test_accuracy)\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train loss: {total_loss:.4f} - Test loss: {test_loss:.4f} - Test accuracy: {test_accuracy:.4f} - LR: {current_lr:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    os.makedirs('pretrained_nets', exist_ok=True)\n",
    "    if best_model is not None:\n",
    "        print(f\"Saving best model at epoch {np.argmin(loss_test) + 1} and test loss {best_loss:.4f}\")\n",
    "        torch.save(best_model, f\"pretrained_nets/{name}.ckpt\")\n",
    "    else:\n",
    "        print(\"No best model found, saving current model\")\n",
    "        torch.save(model.state_dict(), f\"pretrained_nets/{name}.ckpt\")\n",
    "\n",
    "    return loss_train, loss_test, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 276186,
     "status": "ok",
     "timestamp": 1730886949183,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "x-aP92V_jW_F",
    "outputId": "136933a3-18dc-47af-b7ed-4372e743d3b7"
   },
   "outputs": [],
   "source": [
    "# Train domain classifier on Df\n",
    "\n",
    "if dataset == 'realworld':\n",
    "    dataset_name = 'realworld_128_3ch_4cl'\n",
    "    num_df_domains = 10\n",
    "    num_dp_domains = 5\n",
    "    num_classes = 4\n",
    "\n",
    "elif dataset == 'cwru':\n",
    "    dataset_name = 'cwru_256_3ch_5cl'\n",
    "    num_df_domains = 4\n",
    "    num_dp_domains = 4\n",
    "    num_classes = 5\n",
    "\n",
    "elif dataset == 'realworld_mobiact':\n",
    "    dataset_name = 'realworld_mobiact'\n",
    "    num_df_domains = 15\n",
    "    num_dp_domains = 61\n",
    "    num_classes = 4\n",
    "\n",
    "\n",
    "with open(f'data/{dataset_name}.pkl', 'rb') as f:\n",
    "    x, y, k = pickle.load(f)\n",
    "\n",
    "x = x[k < num_df_domains]\n",
    "y = y[k < num_df_domains]\n",
    "k = k[k < num_df_domains]\n",
    "print(x.shape, y.shape, k.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test, k_train, k_test = split_data(x, y, k, test_size=0.2, random_state=seed)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, k_train.shape, k_test.shape)\n",
    "\n",
    "model = MultiBranchClassifier(num_domains=num_df_domains, num_classes=num_classes, num_timesteps=x_train.shape[2])\n",
    "\n",
    "train_loader, test_loader = setup_training(x_train, y_train, k_train, x_test, y_test, k_test)\n",
    "\n",
    "initial_lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "loss_train, loss_test, accuracy_test = train_model(model, train_loader, test_loader, optimizer, epochs=300, name=f'domain_classifier_{dataset}_df')\n",
    "\n",
    "# Find the best epoch based on the test loss\n",
    "best_epoch = np.argmin(loss_test)\n",
    "print(f\"Best epoch: {best_epoch + 1} - Test loss: {loss_test[best_epoch]:.4f} - Test accuracy: {accuracy_test[best_epoch]:.4f}\")\n",
    "\n",
    "# Plot the training and test loss displaying the best epoch with a vertical line\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(loss_train, label='Train loss')\n",
    "plt.plot(loss_test, label='Test loss')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the test accuracy\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(accuracy_test, label='Test accuracy')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 219267,
     "status": "ok",
     "timestamp": 1730887218753,
     "user": {
      "displayName": "Giovanna Pasini",
      "userId": "02341854514736870674"
     },
     "user_tz": -60
    },
    "id": "6d1Wc15QjW_F",
    "outputId": "7cc7e12f-48a7-423d-b8d0-f44d07dfba1e"
   },
   "outputs": [],
   "source": [
    "# Train domain classifier on Dp\n",
    "\n",
    "with open(f'data/{dataset_name}.pkl', 'rb') as f:\n",
    "    x, y, k = pickle.load(f)\n",
    "\n",
    "with open(f'data/{dataset_name}_fs.pkl', 'rb') as f:\n",
    "    fs = pickle.load(f)\n",
    "\n",
    "x = x[fs == 0]\n",
    "y = y[fs == 0]\n",
    "k = k[fs == 0]\n",
    "x = x[k >= num_df_domains]\n",
    "y = y[k >= num_df_domains]\n",
    "k = k[k >= num_df_domains] - num_df_domains\n",
    "print(x.shape, y.shape, k.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test, k_train, k_test = split_data(x, y, k, test_size=0.2, random_state=seed)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, k_train.shape, k_test.shape)\n",
    "\n",
    "model = MultiBranchClassifier(num_domains=num_dp_domains, num_classes=num_classes, num_timesteps=x_train.shape[2])\n",
    "\n",
    "train_loader, test_loader = setup_training(x_train, y_train, k_train, x_test, y_test, k_test)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "initial_lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "loss_train, loss_test, accuracy_test = train_model(model, train_loader, test_loader, optimizer, epochs=300, name=f'domain_classifier_{dataset}_dp')\n",
    "\n",
    "# Find the best epoch based on the test loss\n",
    "best_epoch = np.argmin(loss_test)\n",
    "print(f\"Best epoch: {best_epoch + 1} - Test loss: {loss_test[best_epoch]:.4f} - Test accuracy: {accuracy_test[best_epoch]:.4f}\")\n",
    "\n",
    "# Plot the training and test loss displaying the best epoch with a vertical line\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(loss_train, label='Train loss')\n",
    "plt.plot(loss_test, label='Test loss')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the test accuracy\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(accuracy_test, label='Test accuracy')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppaMwhCOjW_G"
   },
   "source": [
    "## Siamese Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnjciukDjW_G"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "seed = 2710\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class MultiBranchSiameseNet(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_classes=5, num_timesteps=128):\n",
    "        super(MultiBranchSiameseNet, self).__init__()\n",
    "        # Shared layers\n",
    "        self.conv1 = nn.Conv1d(num_channels, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc_shared = nn.Linear(num_timesteps * 8, 100)\n",
    "\n",
    "        # Class-specific branches\n",
    "        self.fc_class_branches = nn.Linear(100, 50 * num_classes)\n",
    "\n",
    "    def forward_once(self, x, class_id):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc_shared(x))\n",
    "        print(x.size())\n",
    "\n",
    "        # Process class-specific branch\n",
    "        class_branches = self.fc_class_branches(x).view(x.size(0), -1, 50)\n",
    "        print(class_branches.size())\n",
    "        class_output = class_branches[torch.arange(class_branches.size(0)), class_id]\n",
    "        print(class_output.size())\n",
    "        return class_output\n",
    "\n",
    "\n",
    "def split_data(x, y, k, test_size=0.2, random_state=seed):\n",
    "    x_train, x_test, y_train, y_test, k_train, k_test = train_test_split(x, y, k,\n",
    "                                                                         test_size=test_size,\n",
    "                                                                         random_state=random_state,\n",
    "                                                                         stratify=k,\n",
    "                                                                         shuffle=True)\n",
    "    return x_train, x_test, y_train, y_test, k_train, k_test\n",
    "\n",
    "\n",
    "def setup_training(x_train, y_train, k_train, x_test, y_test, k_test, batch_size=64):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    k_train_tensor = torch.tensor(k_train, dtype=torch.long)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    k_test_tensor = torch.tensor(k_test, dtype=torch.long)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor, k_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor, k_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu', margin=1.0):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    same_domain_distance = 0\n",
    "    diff_domain_distance = 0\n",
    "    same_domain_count = 0\n",
    "    diff_domain_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, k_batch in test_loader:\n",
    "            x_batch, y_batch, k_batch = x_batch.to(device), y_batch.to(device), k_batch.to(device)\n",
    "            batch_size = x_batch.size(0)\n",
    "\n",
    "            # Compute all pairwise outputs\n",
    "            output1 = model.forward_once(x_batch.unsqueeze(1).expand(-1, batch_size, -1, -1).reshape(-1, x_batch.size(1), x_batch.size(2)),\n",
    "                                         y_batch.unsqueeze(1).expand(-1, batch_size).reshape(-1))\n",
    "            output2 = model.forward_once(x_batch.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(-1, x_batch.size(1), x_batch.size(2)),\n",
    "                                         y_batch.unsqueeze(0).expand(batch_size, -1).reshape(-1))\n",
    "\n",
    "            output1 = output1.view(batch_size, batch_size, -1)\n",
    "            output2 = output2.view(batch_size, batch_size, -1)\n",
    "\n",
    "            # Compute pairwise distances\n",
    "            euclidean_distances = F.pairwise_distance(output1, output2, keepdim=True).view(batch_size, batch_size)\n",
    "\n",
    "            # Compute pairwise labels\n",
    "            labels = (k_batch.unsqueeze(1) != k_batch.unsqueeze(0)).float()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = (1 - labels) * torch.pow(euclidean_distances, 2) + labels * torch.pow(torch.clamp(margin - euclidean_distances, min=0.0), 2)\n",
    "            total_loss += loss.mean().item()\n",
    "\n",
    "            # Compute distances for same and different domains\n",
    "            same_domain_distance += euclidean_distances[labels == 0].sum().item()\n",
    "            diff_domain_distance += euclidean_distances[labels == 1].sum().item()\n",
    "            same_domain_count += (labels == 0).sum().item()\n",
    "            diff_domain_count += (labels == 1).sum().item()\n",
    "\n",
    "    total_loss /= len(test_loader)\n",
    "    avg_same_domain_distance = same_domain_distance / same_domain_count if same_domain_count > 0 else 0\n",
    "    avg_diff_domain_distance = diff_domain_distance / diff_domain_count if diff_domain_count > 0 else 0\n",
    "\n",
    "    return total_loss, avg_same_domain_distance, avg_diff_domain_distance\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, epochs=300, name='siamese_net', margin=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    avg_same_domain_distance_test = []\n",
    "    avg_diff_domain_distance_test = []\n",
    "    best_loss = np.inf\n",
    "\n",
    "    # Set up linear learning rate decay\n",
    "    lambda_lr = lambda epoch: 1 - epoch / epochs\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch, k_batch in train_loader:\n",
    "            x_batch, y_batch, k_batch = x_batch.to(device), y_batch.to(device), k_batch.to(device)\n",
    "            batch_size = x_batch.size(0)\n",
    "\n",
    "            # Compute all pairwise outputs\n",
    "            output1 = model.forward_once(x_batch.unsqueeze(1).expand(-1, batch_size, -1, -1).reshape(-1, x_batch.size(1), x_batch.size(2)),\n",
    "                                         y_batch.unsqueeze(1).expand(-1, batch_size).reshape(-1))\n",
    "            output2 = model.forward_once(x_batch.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(-1, x_batch.size(1), x_batch.size(2)),\n",
    "                                         y_batch.unsqueeze(0).expand(batch_size, -1).reshape(-1))\n",
    "\n",
    "            output1 = output1.view(batch_size, batch_size, -1)\n",
    "            output2 = output2.view(batch_size, batch_size, -1)\n",
    "\n",
    "            # Compute pairwise distances\n",
    "            euclidean_distances = F.pairwise_distance(output1, output2, keepdim=True).view(batch_size, batch_size)\n",
    "\n",
    "            # Compute pairwise labels\n",
    "            labels = (k_batch.unsqueeze(1) != k_batch.unsqueeze(0)).float()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = (1 - labels) * torch.pow(euclidean_distances, 2) + labels * torch.pow(torch.clamp(margin - euclidean_distances, min=0.0), 2)\n",
    "            total_loss += loss.mean().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss /= len(train_loader)\n",
    "        loss_train.append(total_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        test_loss, avg_same_domain_distance, avg_diff_domain_distance = evaluate_model(model, test_loader, device=device)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "        loss_test.append(test_loss)\n",
    "        avg_same_domain_distance_test.append(avg_same_domain_distance)\n",
    "        avg_diff_domain_distance_test.append(avg_diff_domain_distance)\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train loss: {total_loss:.4f} - Test loss: {test_loss:.4f} - \"\n",
    "              f\"Avg same domain distance: {avg_same_domain_distance:.4f} - Avg diff domain distance: {avg_diff_domain_distance:.4f} - LR: {current_lr:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    os.makedirs('pretrained_nets', exist_ok=True)\n",
    "    if best_model is not None:\n",
    "        print(f\"Saving best model at epoch {np.argmin(loss_test) + 1} and test loss {best_loss:.4f}\")\n",
    "        torch.save(best_model, f\"pretrained_nets/{name}.ckpt\")\n",
    "    else:\n",
    "        print(\"No best model found, saving current model\")\n",
    "        torch.save(model.state_dict(), f\"pretrained_nets/{name}.ckpt\")\n",
    "\n",
    "    return loss_train, loss_test, avg_same_domain_distance_test, avg_diff_domain_distance_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLnLicnGjW_H"
   },
   "outputs": [],
   "source": [
    "# Train Siamese network on Df\n",
    "\n",
    "if dataset == 'realworld':\n",
    "    dataset_name = 'realworld'\n",
    "    num_df_domains = 10\n",
    "    num_dp_domains = 5\n",
    "    num_classes = 4\n",
    "\n",
    "elif dataset == 'cwru':\n",
    "    dataset_name = 'cwru_256_3ch_5cl'\n",
    "    num_df_domains = 4\n",
    "    num_dp_domains = 4\n",
    "    num_classes = 5\n",
    "\n",
    "elif dataset == 'realworld_mobiact':\n",
    "    dataset_name = 'realworld_mobiact'\n",
    "    num_df_domains = 15\n",
    "    num_dp_domains = 61\n",
    "    num_classes = 4\n",
    "\n",
    "\n",
    "with open(f'data/{dataset_name}.pkl', 'rb') as f:\n",
    "    x, y, k = pickle.load(f)\n",
    "\n",
    "x = x[k < num_df_domains]\n",
    "y = y[k < num_df_domains]\n",
    "k = k[k < num_df_domains]\n",
    "print(x.shape, y.shape, k.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test, k_train, k_test = split_data(x, y, k, test_size=0.2, random_state=seed)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, k_train.shape, k_test.shape)\n",
    "\n",
    "model = MultiBranchSiameseNet(num_classes=num_classes, num_timesteps=x_train.shape[2])\n",
    "\n",
    "train_loader, test_loader = setup_training(x_train, y_train, k_train, x_test, y_test, k_test)\n",
    "\n",
    "initial_lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "loss_train, loss_test, avg_same_domain_distance_test, avg_diff_domain_distance_test = train_model(model, train_loader, test_loader, optimizer, epochs=200, name=f'siamese_net_{dataset}_df')\n",
    "\n",
    "# Find the best epoch based on the test loss\n",
    "best_epoch = np.argmin(loss_test)\n",
    "print(f\"Best epoch: {best_epoch + 1} - Test loss: {loss_test[best_epoch]:.4f} - Avg same domain distance: {avg_same_domain_distance_test[best_epoch]:.4f} - Avg diff domain distance: {avg_diff_domain_distance_test[best_epoch]:.4f}\")\n",
    "\n",
    "# Plot the training and test loss displaying the best epoch with a vertical line\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(loss_train, label='Train loss')\n",
    "plt.plot(loss_test, label='Test loss')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the average same domain distance\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(avg_same_domain_distance_test, label='Avg same domain distance')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg Same Domain Distance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the average different domain distance\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(avg_diff_domain_distance_test, label='Avg diff domain distance')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg Diff Domain Distance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvqpOhZkjW_I"
   },
   "outputs": [],
   "source": [
    "# Train Siamese Network on Dp\n",
    "\n",
    "with open(f'data/{dataset_name}.pkl', 'rb') as f:\n",
    "    x, y, k = pickle.load(f)\n",
    "\n",
    "with open(f'data/{dataset_name}_fs.pkl', 'rb') as f:\n",
    "    fs = pickle.load(f)\n",
    "\n",
    "x = x[fs == 0]\n",
    "y = y[fs == 0]\n",
    "k = k[fs == 0]\n",
    "x = x[k >= num_df_domains]\n",
    "y = y[k >= num_df_domains]\n",
    "k = k[k >= num_df_domains] - num_df_domains\n",
    "print(x.shape, y.shape, k.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test, k_train, k_test = split_data(x, y, k, test_size=0.2, random_state=seed)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, k_train.shape, k_test.shape)\n",
    "\n",
    "model = MultiBranchSiameseNet(num_classes=num_classes, num_timesteps=x_train.shape[2])\n",
    "\n",
    "train_loader, test_loader = setup_training(x_train, y_train, k_train, x_test, y_test, k_test)\n",
    "\n",
    "initial_lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "loss_train, loss_test, avg_same_domain_distance_test, avg_diff_domain_distance_test = train_model(model, train_loader, test_loader, optimizer, epochs=200, name=f'siamese_net_{dataset}_dp')\n",
    "\n",
    "# Find the best epoch based on the test loss\n",
    "best_epoch = np.argmin(loss_test)\n",
    "print(f\"Best epoch: {best_epoch + 1} - Test loss: {loss_test[best_epoch]:.4f} - Avg same domain distance: {avg_same_domain_distance_test[best_epoch]:.4f} - Avg diff domain distance: {avg_diff_domain_distance_test[best_epoch]:.4f}\")\n",
    "\n",
    "# Plot the training and test loss displaying the best epoch with a vertical line\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(loss_train, label='Train loss')\n",
    "plt.plot(loss_test, label='Test loss')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the average same domain distance\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(avg_same_domain_distance_test, label='Avg same domain distance')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg Same Domain Distance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the average different domain distance\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(avg_diff_domain_distance_test, label='Avg diff domain distance')\n",
    "plt.axvline(best_epoch, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg Diff Domain Distance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "stargan-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
