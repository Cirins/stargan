{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max Accelerometer Frequency: 50.1 Hz\n",
      "Min Accelerometer Frequency: 49.94 Hz\n",
      "\n",
      "Max Gyroscope Frequency: 50.1 Hz\n",
      "Min Gyroscope Frequency: 49.94 Hz\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_frequency(base_dir, location, activities):\n",
    "    acc_frequencies = []\n",
    "    gyr_frequencies = []\n",
    "    \n",
    "    # Adjusted regex pattern to account for variability in formatting\n",
    "    frequency_pattern = re.compile(rf'{location}\\.csv.*?frequency:\\s*(\\d+\\.\\d+)\\s*Hz', re.DOTALL)\n",
    "\n",
    "    for i in range(1, 16):  # Assuming proband1 through proband15\n",
    "        for activity in activities:\n",
    "            # Paths to the accelerometer and gyroscope zip files for each activity\n",
    "            acc_zip_path = os.path.join(base_dir, f'proband{i}', 'data', f'acc_{activity}_csv.zip')\n",
    "            gyr_zip_path = os.path.join(base_dir, f'proband{i}', 'data', f'gyr_{activity}_csv.zip')\n",
    "\n",
    "            # Extract and process accelerometer frequency\n",
    "            if os.path.exists(acc_zip_path):\n",
    "                with zipfile.ZipFile(acc_zip_path, 'r') as zip_ref:\n",
    "                    if 'readMe' in zip_ref.namelist():\n",
    "                        with zip_ref.open('readMe') as readme:\n",
    "                            content = readme.read().decode()\n",
    "                            match = frequency_pattern.search(content)\n",
    "                            if match:\n",
    "                                acc_frequencies.append(float(match.group(1)))\n",
    "                                # print(f\"Extracted ACC frequency for proband{i}, {activity}: {match.group(1)} Hz\")\n",
    "                            else:\n",
    "                                print(f\"Could not find ACC {location} frequency for proband{i}, {activity}.\")\n",
    "\n",
    "            # Extract and process gyroscope frequency\n",
    "            if os.path.exists(gyr_zip_path):\n",
    "                with zipfile.ZipFile(gyr_zip_path, 'r') as zip_ref:\n",
    "                    if 'readMe' in zip_ref.namelist():\n",
    "                        with zip_ref.open('readMe') as readme:\n",
    "                            content = readme.read().decode()\n",
    "                            match = frequency_pattern.search(content)\n",
    "                            if match:\n",
    "                                gyr_frequencies.append(float(match.group(1)))\n",
    "                                # print(f\"Extracted GYR frequency for proband{i}, {activity}: {match.group(1)} Hz\")\n",
    "                            else:\n",
    "                                print(f\"Could not find GYR {location} frequency for proband{i}, {activity}.\")\n",
    "\n",
    "    return acc_frequencies, gyr_frequencies\n",
    "\n",
    "# Base directory containing the proband subdirectories\n",
    "base_dir = 'RealWorld'\n",
    "\n",
    "location = 'waist'\n",
    "activities = ['walking', 'running', 'standing', 'climbingdown', 'climbingup']\n",
    "\n",
    "# Extract frequencies\n",
    "acc_frequencies, gyr_frequencies = extract_frequency(base_dir, location, activities)\n",
    "\n",
    "print()\n",
    "# print(f\"Accelerometer Frequencies: {acc_frequencies}\")\n",
    "print(f\"Max Accelerometer Frequency: {max(acc_frequencies)} Hz\")\n",
    "print(f\"Min Accelerometer Frequency: {min(acc_frequencies)} Hz\")\n",
    "print()\n",
    "# print(f\"Gyroscope Frequencies: {gyr_frequencies}\")\n",
    "print(f\"Max Gyroscope Frequency: {max(gyr_frequencies)} Hz\")\n",
    "print(f\"Min Gyroscope Frequency: {min(gyr_frequencies)} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "activity_full_names = {\n",
    "    'WAL': 'walking',\n",
    "    'RUN': 'running',\n",
    "    'STN': 'standing',\n",
    "    'CLD': 'climbingdown',\n",
    "    'CLU': 'climbingup'\n",
    "}\n",
    "\n",
    "def extract_data(zip_path, activity_name, location):\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"\\t{zip_path} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        csv_file_name_1 = f'acc_{activity_name}_{location}.csv'\n",
    "        csv_file_name_2 = f'acc_{activity_name}_2_{location}.csv'\n",
    "        csv_file_name_3 = f'acc_{activity_name}_3_{location}.csv'\n",
    "        if csv_file_name_1 in z.namelist():\n",
    "            with z.open(csv_file_name_1) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                # print(f'Extracted {csv_file_name_1} from {zip_path}.')\n",
    "                return df\n",
    "        elif csv_file_name_2 in z.namelist():\n",
    "            with z.open(csv_file_name_2) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                # print(f'Extracted {csv_file_name_2} from {zip_path}.')\n",
    "                return df\n",
    "        elif csv_file_name_3 in z.namelist():\n",
    "            with z.open(csv_file_name_3) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                # print(f'Extracted {csv_file_name_3} from {zip_path}.')\n",
    "                return df\n",
    "        \n",
    "        # If the CSV isn't in the main ZIP, look for the second ZIP and access it directly\n",
    "        inner_zip_file = f'acc_{activity_name}_1_csv.zip'\n",
    "        if inner_zip_file in z.namelist():\n",
    "            with z.open(inner_zip_file) as inner_z_file:\n",
    "                with zipfile.ZipFile(inner_z_file) as inner_z:\n",
    "                    if csv_file_name_1 in inner_z.namelist():\n",
    "                        with inner_z.open(csv_file_name_1) as f:\n",
    "                            df = pd.read_csv(f)\n",
    "                            # print(f'Extracted {csv_file_name_1} from {inner_zip_file}.')\n",
    "                            return df\n",
    "\n",
    "    print(f\"{csv_file_name_1} not found in {zip_path} or {inner_zip_file}.\")\n",
    "    return None \n",
    "\n",
    "def extract_all_data(activity_full_names, location):\n",
    "    data_dict = {}\n",
    "    for i in range(1, 16):\n",
    "        subjectID = f'proband{i}'\n",
    "        for activity_code, activity_name in activity_full_names.items():\n",
    "            zip_path = os.path.join('RealWorld', subjectID, 'data', f'acc_{activity_name}_csv.zip')\n",
    "            # print(f\"Extracting data for {location}, {activity_code.upper()}, subject {subjectID}...\")\n",
    "            df = extract_data(zip_path, activity_name, location)\n",
    "            if df is not None:\n",
    "                data = np.array(df[['attr_x', 'attr_y', 'attr_z']].values)\n",
    "                if activity_code not in data_dict:\n",
    "                    data_dict[activity_code] = {}\n",
    "                if i-1 not in data_dict[activity_code]:\n",
    "                    data_dict[activity_code][i-1] = []\n",
    "                data_dict[activity_code][i-1].append(data)\n",
    "            else:\n",
    "                print(f\"\\tData for {location}, {activity_code.upper()}, subject {subjectID} could not be fully extracted.\")\n",
    "    print(\"Data successfully extracted.\")\n",
    "    return data_dict\n",
    "\n",
    "data_dict = extract_all_data(activity_full_names, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity: WAL, Number of subjects: 15\n",
      "Activity: RUN, Number of subjects: 15\n",
      "Activity: STN, Number of subjects: 15\n",
      "Activity: CLD, Number of subjects: 15\n",
      "Activity: CLU, Number of subjects: 15\n"
     ]
    }
   ],
   "source": [
    "for activity in data_dict.keys():\n",
    "    print(f\"Activity: {activity}, Number of subjects: {len([data_dict[activity][i] for i in data_dict[activity].keys()])}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity: WAL, Subject: 0, Shape: (31948, 3)\n",
      "Activity: WAL, Subject: 1, Shape: (30747, 3)\n",
      "Activity: WAL, Subject: 2, Shape: (34118, 3)\n",
      "Activity: WAL, Subject: 3, Shape: (31031, 3)\n",
      "Activity: WAL, Subject: 4, Shape: (34837, 3)\n",
      "Activity: WAL, Subject: 5, Shape: (31561, 3)\n",
      "Activity: WAL, Subject: 6, Shape: (30906, 3)\n",
      "Activity: WAL, Subject: 7, Shape: (32909, 3)\n",
      "Activity: WAL, Subject: 8, Shape: (31396, 3)\n",
      "Activity: WAL, Subject: 9, Shape: (31770, 3)\n",
      "Activity: WAL, Subject: 10, Shape: (33299, 3)\n",
      "Activity: WAL, Subject: 11, Shape: (31750, 3)\n",
      "Activity: WAL, Subject: 12, Shape: (32794, 3)\n",
      "Activity: WAL, Subject: 13, Shape: (33578, 3)\n",
      "Activity: WAL, Subject: 14, Shape: (33218, 3)\n",
      "Activity: RUN, Subject: 0, Shape: (30628, 3)\n",
      "Activity: RUN, Subject: 1, Shape: (30654, 3)\n",
      "Activity: RUN, Subject: 2, Shape: (37939, 3)\n",
      "Activity: RUN, Subject: 3, Shape: (52178, 3)\n",
      "Activity: RUN, Subject: 4, Shape: (55648, 3)\n",
      "Activity: RUN, Subject: 5, Shape: (33268, 3)\n",
      "Activity: RUN, Subject: 6, Shape: (36775, 3)\n",
      "Activity: RUN, Subject: 7, Shape: (30871, 3)\n",
      "Activity: RUN, Subject: 8, Shape: (40943, 3)\n",
      "Activity: RUN, Subject: 9, Shape: (32220, 3)\n",
      "Activity: RUN, Subject: 10, Shape: (31086, 3)\n",
      "Activity: RUN, Subject: 11, Shape: (30978, 3)\n",
      "Activity: RUN, Subject: 12, Shape: (30983, 3)\n",
      "Activity: RUN, Subject: 13, Shape: (31041, 3)\n",
      "Activity: RUN, Subject: 14, Shape: (33795, 3)\n",
      "Activity: STN, Subject: 0, Shape: (32119, 3)\n",
      "Activity: STN, Subject: 1, Shape: (30771, 3)\n",
      "Activity: STN, Subject: 2, Shape: (31221, 3)\n",
      "Activity: STN, Subject: 3, Shape: (30242, 3)\n",
      "Activity: STN, Subject: 4, Shape: (30042, 3)\n",
      "Activity: STN, Subject: 5, Shape: (31815, 3)\n",
      "Activity: STN, Subject: 6, Shape: (33118, 3)\n",
      "Activity: STN, Subject: 7, Shape: (32150, 3)\n",
      "Activity: STN, Subject: 8, Shape: (32169, 3)\n",
      "Activity: STN, Subject: 9, Shape: (33069, 3)\n",
      "Activity: STN, Subject: 10, Shape: (31481, 3)\n",
      "Activity: STN, Subject: 11, Shape: (30871, 3)\n",
      "Activity: STN, Subject: 12, Shape: (34041, 3)\n",
      "Activity: STN, Subject: 13, Shape: (31190, 3)\n",
      "Activity: STN, Subject: 14, Shape: (31471, 3)\n",
      "Activity: CLD, Subject: 0, Shape: (25428, 3)\n",
      "Activity: CLD, Subject: 1, Shape: (24826, 3)\n",
      "Activity: CLD, Subject: 2, Shape: (27743, 3)\n",
      "Activity: CLD, Subject: 3, Shape: (11089, 3)\n",
      "Activity: CLD, Subject: 4, Shape: (25342, 3)\n",
      "Activity: CLD, Subject: 5, Shape: (24702, 3)\n",
      "Activity: CLD, Subject: 6, Shape: (9673, 3)\n",
      "Activity: CLD, Subject: 7, Shape: (22100, 3)\n",
      "Activity: CLD, Subject: 8, Shape: (25016, 3)\n",
      "Activity: CLD, Subject: 9, Shape: (21939, 3)\n",
      "Activity: CLD, Subject: 10, Shape: (24903, 3)\n",
      "Activity: CLD, Subject: 11, Shape: (24378, 3)\n",
      "Activity: CLD, Subject: 12, Shape: (21930, 3)\n",
      "Activity: CLD, Subject: 13, Shape: (7333, 3)\n",
      "Activity: CLD, Subject: 14, Shape: (25601, 3)\n",
      "Activity: CLU, Subject: 0, Shape: (32864, 3)\n",
      "Activity: CLU, Subject: 1, Shape: (25152, 3)\n",
      "Activity: CLU, Subject: 2, Shape: (29552, 3)\n",
      "Activity: CLU, Subject: 3, Shape: (11536, 3)\n",
      "Activity: CLU, Subject: 4, Shape: (30431, 3)\n",
      "Activity: CLU, Subject: 5, Shape: (25950, 3)\n",
      "Activity: CLU, Subject: 6, Shape: (11257, 3)\n",
      "Activity: CLU, Subject: 7, Shape: (57860, 3)\n",
      "Activity: CLU, Subject: 8, Shape: (27310, 3)\n",
      "Activity: CLU, Subject: 9, Shape: (22968, 3)\n",
      "Activity: CLU, Subject: 10, Shape: (31465, 3)\n",
      "Activity: CLU, Subject: 11, Shape: (28017, 3)\n",
      "Activity: CLU, Subject: 12, Shape: (30062, 3)\n",
      "Activity: CLU, Subject: 13, Shape: (11588, 3)\n",
      "Activity: CLU, Subject: 14, Shape: (29170, 3)\n"
     ]
    }
   ],
   "source": [
    "for activity in data_dict.keys():\n",
    "    for subject in data_dict[activity].keys():\n",
    "        print(f\"Activity: {activity}, Subject: {subject}, Shape: {data_dict[activity][subject][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_series(data_dict, activity, subject):\n",
    "    data_list = data_dict[activity][subject]\n",
    "    for i, data in enumerate(data_list):\n",
    "        plt.figure(figsize=(30, 2))\n",
    "        plt.plot(data, linewidth=0.5)\n",
    "        plt.title(f'Activity: {activity}, Subject: {subject}, Observation: {i}')\n",
    "        plt.show()\n",
    "\n",
    "# for activity in data_dict.keys():\n",
    "    # plot_time_series(data_dict, activity, 0)\n",
    "    # plot_time_series(data_dict, activity, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_ts(data_dict, subject, activity):\n",
    "    ts = data_dict[activity][subject][0]\n",
    "    data_dict[activity][subject][0] = -ts\n",
    "\n",
    "flip_ts(data_dict, 2, 'WAL')\n",
    "flip_ts(data_dict, 2, 'RUN')\n",
    "flip_ts(data_dict, 2, 'STN')\n",
    "flip_ts(data_dict, 2, 'CLD')\n",
    "flip_ts(data_dict, 2, 'CLU')\n",
    "flip_ts(data_dict, 7, 'CLU')\n",
    "\n",
    "# for activity in data_dict.keys():\n",
    "    # plot_time_series(data_dict, activity, 0)\n",
    "    # plot_time_series(data_dict, activity, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 2710\n",
    "np.random.seed(seed)\n",
    "\n",
    "def random_rotation_matrix():\n",
    "    # Generate random angles in radians\n",
    "    alpha = np.random.uniform(0, 2 * np.pi)\n",
    "    beta = np.random.uniform(0, 2 * np.pi)\n",
    "    gamma = np.random.uniform(0, 2 * np.pi)\n",
    "    \n",
    "    # Rotation matrix around x-axis\n",
    "    R_x = np.array([[1, 0, 0],\n",
    "                    [0, np.cos(alpha), -np.sin(alpha)],\n",
    "                    [0, np.sin(alpha), np.cos(alpha)]])\n",
    "    \n",
    "    # Rotation matrix around y-axis\n",
    "    R_y = np.array([[np.cos(beta), 0, np.sin(beta)],\n",
    "                    [0, 1, 0],\n",
    "                    [-np.sin(beta), 0, np.cos(beta)]])\n",
    "    \n",
    "    # Rotation matrix around z-axis\n",
    "    R_z = np.array([[np.cos(gamma), -np.sin(gamma), 0],\n",
    "                    [np.sin(gamma), np.cos(gamma), 0],\n",
    "                    [0, 0, 1]])\n",
    "    \n",
    "    # Combined rotation matrix\n",
    "    R = np.dot(R_z, np.dot(R_y, R_x))\n",
    "    return R\n",
    "\n",
    "\n",
    "def augment_data(data_dict, n_augmentations=1):\n",
    "    augmented_data_dict = {}\n",
    "    for activity in data_dict.keys():\n",
    "        augmented_data_dict[activity] = {}\n",
    "        for subject in data_dict[activity].keys():\n",
    "            augmented_data_dict[activity][subject] = [data_dict[activity][subject][0]]\n",
    "            for ts in data_dict[activity][subject]:\n",
    "                for i in range(n_augmentations):\n",
    "                    R = random_rotation_matrix()\n",
    "                    augmented_ts = np.dot(ts, R)\n",
    "                    augmented_data_dict[activity][subject].append(augmented_ts)\n",
    "    return augmented_data_dict\n",
    "\n",
    "\n",
    "augmented_data_dict = augment_data(data_dict, n_augmentations=0)\n",
    "\n",
    "# for activity in augmented_data_dict.keys():\n",
    "#     plot_time_series(augmented_data_dict, activity, 0)\n",
    "    # plot_time_series(augmented_data_dict, activity, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Max: 19.61, Global Min: -19.61\n"
     ]
    }
   ],
   "source": [
    "def minmax_normalize(data_dict):\n",
    "    # Concatenate all data points to compute global max and min\n",
    "    all_data = np.concatenate([np.concatenate(data_dict[activity][subject]) for activity in data_dict.keys() for subject in data_dict[activity].keys()])\n",
    "    global_max = all_data.max()\n",
    "    global_min = all_data.min()\n",
    "\n",
    "    print(f\"Global Max: {global_max:.2f}, Global Min: {global_min:.2f}\")\n",
    "    \n",
    "    def normalize(data):\n",
    "        return (data - global_min) / (global_max - global_min)\n",
    "    \n",
    "    normalized_data_dict = {}\n",
    "    for activity in data_dict.keys():\n",
    "        normalized_data_dict[activity] = {}\n",
    "        for subject in data_dict[activity].keys():\n",
    "            normalized_data_dict[activity][subject] = []\n",
    "            for ts in data_dict[activity][subject]:\n",
    "                normalized_data_dict[activity][subject].append(normalize(ts))\n",
    "    return normalized_data_dict\n",
    "\n",
    "normalized_data_dict = minmax_normalize(augmented_data_dict)\n",
    "\n",
    "# for activity in normalized_data_dict.keys():\n",
    "#     plot_time_series(normalized_data_dict, activity, 0)\n",
    "#     plot_time_series(normalized_data_dict, activity, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_dict = {\n",
    "    'WAL': (600, 1000),\n",
    "    'RUN': (750, 1500),\n",
    "    'STN': (0, 1),\n",
    "    'CLD': (1200, 750),\n",
    "    'CLU': (1400, 1400),\n",
    "    # 'JMP': (350, 500),\n",
    "    # 'LYI': (1500, 1700),\n",
    "    # 'SIT': (1750, 1250),\n",
    "}\n",
    "\n",
    "def trim_time_series(data_dict, trim_dict):\n",
    "    new_data_dict = {}\n",
    "    for activity in data_dict.keys():\n",
    "        new_data_dict[activity] = {}\n",
    "        for subject in data_dict[activity].keys():\n",
    "            new_data_dict[activity][subject] = []\n",
    "            start, end = trim_dict[activity]\n",
    "            for ts in data_dict[activity][subject]:\n",
    "                new_data_dict[activity][subject].append(ts[start:-end])\n",
    "\n",
    "    return new_data_dict\n",
    "\n",
    "trim_data_dict = trim_time_series(normalized_data_dict, trim_dict)\n",
    "\n",
    "# for activity in trim_data_dict.keys():\n",
    "#     plot_time_series(trim_data_dict, activity, 0)\n",
    "#     plot_time_series(trim_data_dict, activity, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_time_series_more(data_dict, trim_dict):\n",
    "    new_data_dict = {}\n",
    "\n",
    "    for activity in data_dict.keys():\n",
    "        new_data_dict[activity] = {}\n",
    "        for subject in data_dict[activity].keys():\n",
    "            trim_ranges = trim_dict[activity][subject]\n",
    "            # Sort trim ranges by the first element of each tuple\n",
    "            trim_ranges.sort(key=lambda x: x[0])\n",
    "            \n",
    "            # Initialize the start index of the time series to be trimmed\n",
    "            start_idx = 0\n",
    "            new_series_list = []\n",
    "\n",
    "            for ts in data_dict[activity][subject]:\n",
    "            \n",
    "                for (x1, x2) in trim_ranges:\n",
    "                    # Append the time series from the last cut to the beginning of the current cut\n",
    "                    if x1 > start_idx:\n",
    "                        new_series_list.append(ts[start_idx:x1])\n",
    "                    # Update the start index to the end of the current cut for the next iteration\n",
    "                    start_idx = x2\n",
    "\n",
    "                # After all cuts, if there's any remaining time series, append it\n",
    "                if start_idx < len(ts):\n",
    "                    new_series_list.append(ts[start_idx:])\n",
    "\n",
    "                # Update the start index for the next time series\n",
    "                start_idx = 0\n",
    "\n",
    "            new_data_dict[activity][subject] = new_series_list\n",
    "\n",
    "    return new_data_dict\n",
    "\n",
    "\n",
    "trim_dict = {\n",
    "    'WAL': [[] for _ in range(15)],\n",
    "    'STN': [[] for _ in range(15)],\n",
    "    'RUN': [[],\n",
    "            [],\n",
    "            [(11000, 19000)],\n",
    "            [(8000, 22500), (25900, 32500), (35000, 46000)],\n",
    "            [(9000, 11500), (17500, 28000), (35000, 49000)],\n",
    "            [(11500, 14000)],\n",
    "            [(19000, 23000)],\n",
    "            [],\n",
    "            [(5000, 16000)],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [(2000, 3000), (27500, 32000)]],\n",
    "    'CLD': [[] for _ in range(3)] + [[(4400, 4900)]] + [[] for _ in range(11)],\n",
    "    'CLU': [[] for _ in range(7)] + [[(23000, 43000)]] + [[] for _ in range(7)],\n",
    "    # 'JMP': [[] for _ in range(15)],\n",
    "    # 'LYI': [[] for _ in range(15)],\n",
    "    # 'SIT': [[] for _ in range(15)],\n",
    "}\n",
    "\n",
    "trimmore_data_dict = trim_time_series_more(trim_data_dict, trim_dict)\n",
    "\n",
    "# for activity in trimmore_data_dict.keys():\n",
    "#     plot_time_series(trimmore_data_dict, activity, 0)\n",
    "#     plot_time_series(trimmore_data_dict, activity, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity in trimmore_data_dict.keys():\n",
    "    for subject in trimmore_data_dict[activity].keys():\n",
    "        print(f\"Activity: {activity}, Subject: {subject}, Number of Observations: {len(trimmore_data_dict[activity][subject])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_series(data_dict):\n",
    "    # Fixed length for splitting time series\n",
    "    split_length = 128\n",
    "\n",
    "    split_data_dict = {}\n",
    "    for activity in data_dict.keys():\n",
    "        split_data_dict[activity] = []\n",
    "        for subject in data_dict[activity]:\n",
    "            subject_ts = data_dict[activity][subject]\n",
    "            split_subject = []\n",
    "            for part in subject_ts:\n",
    "                part_length = len(part)\n",
    "                num_windows = part_length // split_length\n",
    "                for i in range(num_windows):\n",
    "                    split_subject.append(part[i*split_length:(i+1)*split_length])\n",
    "            split_data_dict[activity].append(split_subject)\n",
    "\n",
    "    return split_data_dict\n",
    "\n",
    "split_data_dict = split_time_series(trimmore_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity in split_data_dict.keys():\n",
    "    for i, subject in enumerate(split_data_dict[activity]):\n",
    "        print(f\"Activity: {activity}, Subject: {i+1}, Number of time series: {len(subject)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, activity):\n",
    "    fig, axs = plt.subplots(6, 15, figsize=(30, 8))\n",
    "    fig.suptitle(f'Activity: {activity_full_names[activity]}', fontsize=16)\n",
    "    for i in range(15):\n",
    "        step = len(data[activity][i]) // 6\n",
    "        for j in range(6):\n",
    "            axs[j, i].plot(data[activity][i][j*step], linewidth=0.5)\n",
    "            axs[j, i].get_xaxis().set_visible(False)\n",
    "            axs[j, i].get_yaxis().set_visible(False)\n",
    "            axs[j, i].set_ylim(0, 1)\n",
    "            axs[j, i].set_title(f'Subject: {i}, Part: {j*step}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_data(split_data_dict, 'WAL')\n",
    "plot_data(split_data_dict, 'RUN')\n",
    "plot_data(split_data_dict, 'STN')\n",
    "plot_data(split_data_dict, 'CLD')\n",
    "plot_data(split_data_dict, 'CLU')\n",
    "# plot_data(data_dict, 'JMP')\n",
    "# plot_data(data_dict, 'LYI')\n",
    "# plot_data(data_dict, 'SIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, activities=['WAL', 'RUN', 'STN', 'CLD', 'CLU']):\n",
    "    x = []\n",
    "    y = []\n",
    "    k = []\n",
    "\n",
    "    act_idx = 0\n",
    "    for activity in data.keys():\n",
    "        if activity not in activities:\n",
    "            continue\n",
    "        for sub_idx, subject in enumerate(data[activity]):\n",
    "            for part in subject:\n",
    "                x.append(part)\n",
    "                y.append(act_idx)\n",
    "                k.append(sub_idx)\n",
    "        act_idx += 1\n",
    "\n",
    "    return np.array(x).transpose(0,2,1), np.array(y), np.array(k)\n",
    "\n",
    "x, y, k = create_dataset(split_data_dict, activities=['WAL', 'RUN', 'CLD', 'CLU'])\n",
    "print(f\"X shape: {x.shape}, Y shape: {y.shape}, K shape: {k.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize fs vector with zeros\n",
    "fs = np.zeros_like(k)\n",
    "\n",
    "# Iterate through each unique combination of subject and activity\n",
    "for subject in np.unique(k):\n",
    "    for activity in np.unique(y):\n",
    "        # Find indices matching the current combination\n",
    "        indices = np.where((k == subject) & (y == activity))[0]\n",
    "\n",
    "        # If there are at least 5 samples, randomly select 5\n",
    "        if len(indices) >= 5:\n",
    "            selected_indices = np.random.choice(indices, 5, replace=False)\n",
    "            fs[selected_indices] = 1\n",
    "\n",
    "# Verify the result\n",
    "print(\"Number of 1s in fs:\", np.sum(fs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the data to a pickle file\n",
    "# with open('realworld_znorm.pkl', 'wb') as f:\n",
    "#     pickle.dump((x, y, k), f)\n",
    "\n",
    "# # Save the few-shot vector to a pickle file\n",
    "# with open('realworld_znorm_fs.pkl', 'wb') as f:\n",
    "#     pickle.dump(fs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
